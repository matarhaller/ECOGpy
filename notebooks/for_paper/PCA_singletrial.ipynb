{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate single trial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ShadePlots_elecs_stats as SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-218ee4ac124b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshadeplots_elecs_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/knight/matar/PYTHON/ECOGpy/ShadePlots_elecs_stats.py\u001b[0m in \u001b[0;36mshadeplots_elecs_stats\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_idx_resp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_idx_resp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0meidx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactive_elecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meidx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "SP.shadeplots_elecs_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA outlier rejection\n",
    "* all electrodes have the same number of trials\n",
    "* each feature can have a different number of trials\n",
    "* save separate file for each feature, plus an RT file for each feature to match number of trials dropped\n",
    "* for unsmoothed data\n",
    "* edited for surrogates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(SJdir,'PCA', 'csvs_FINAL', 'mean_traces_all_subjs_dropSR.csv')\n",
    "df_all = pd.read_csv(filename)\n",
    "std_thresh = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outlier rejection of RTs to match outlier rejection of PCA for each param\n",
    "features_to_consider = ['means']\n",
    "\n",
    "subjs, tasks, num_total, num_to_drop, feature_list = [[] for i in range(5)]\n",
    "\n",
    "for s_t in df_all.groupby(['subj','task']):\n",
    "        \n",
    "    subj, task = s_t[0]\n",
    "\n",
    "    #find outlier trials\n",
    "    for f in features_to_consider:\n",
    "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows','unsmoothed', 'csv_files', '_'.join([subj, task, 'RTs']) + '.csv')\n",
    "        df_RT = pd.read_csv(filename)\n",
    "\n",
    "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
    "\n",
    "        outliers = np.where(trials_to_drop)[0]\n",
    "\n",
    "        #drop outliers\n",
    "        df.iloc[outliers] = np.nan\n",
    "        df = df.dropna()\n",
    "\n",
    "        df_RT.iloc[outliers] = np.nan\n",
    "        df_RT = df_RT.dropna()\n",
    "\n",
    "        filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_PCA', 'unsmoothed', '_'.join([subj, task, f + '.csv']))\n",
    "        df.to_csv(filename, index = False)\n",
    "        filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_PCA', 'unsmoothed', '_'.join([subj, task, f + '_RTs.csv'])) #RTs per elec (identical) with correct number of trials\n",
    "        df_RT.to_csv(filename, index = False)\n",
    "\n",
    "        subjs.append(subj)\n",
    "        tasks.append(task)\n",
    "        num_total.append(trials_to_drop.shape[0])\n",
    "        num_to_drop.append(len(outliers))\n",
    "        feature_list.append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a summary\n",
    "df_tally = pd.DataFrame({'subj':subjs, 'task': tasks, 'total' : num_total, 'outliers' : num_to_drop, 'feature':feature_list})\n",
    "df_tally['percent_dropped'] = df_tally['outliers']/df_tally['total'] * 100\n",
    "df_tally = df_tally[['subj','task','feature', 'outliers','total','percent_dropped']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers', 'for_PCA','unsmoothed', 'tally_all.csv')\n",
    "df_tally.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine all of df_thresh into 1 csv\n",
    "* thresholded PC loading matrices (from PCA_thresh.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#params = ['means', 'medians', 'maxes', 'maxes_rel', 'stds']\n",
    "params = ['means']\n",
    "\n",
    "filename = os.path.join(SJdir,'PCA', 'csvs_FINAL', 'mean_traces_all_subjs_dropSR.csv')\n",
    "df_pattern = pd.read_csv(os.path.join(filename))\n",
    "\n",
    "subj_task = df_pattern[['subj', 'task']].drop_duplicates()\n",
    "\n",
    "for param in params:\n",
    "    df_all = pd.DataFrame({'subj':[],'task':[], 'elec':[], 'pattern':[], 'ROI':[], 'max_pc':[], 'pc_list':[]})\n",
    "\n",
    "    for s_t in subj_task.itertuples():\n",
    "        _, subj, task = s_t\n",
    "        filename = os.path.join(SJdir, 'PCA','Stats','Networks','zscore','PCA_'+ param,'networks', '_'.join([subj, task,'thresh.csv']))\n",
    "        if os.path.isfile(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            df= df[['subj','task','elec','pattern','ROI','max_pc','pc_list']]\n",
    "            df_all = df_all.append(df)\n",
    "    df_all = df_all[['subj','task','elec','ROI','pattern','max_pc','pc_list']]\n",
    "    filename = os.path.join(SJdir, 'PCA','Stats', 'Networks', 'zscore', 'PCA_'+ param, 'networks', 'all_subjects_thresh_'+ param +'.csv')\n",
    "    df_all.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/Networks/zscore/PCA_means/networks/all_subjects_thresh_means.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold correlation matrices based on threshholded PC1 \n",
    "* (or PC2)\n",
    "* use PCA_threhsold.py using loading matrices from PYTHON/PCA_R/PCA_elecs.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/Networks/zscore/PCA_means/networks/CP7_DecisionAud_thresh.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8ad80a4e2a17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSJdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PCA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Stats'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Networks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zscore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PCA_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'networks'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'thresh.csv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdf_thresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSJdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PCA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Stats'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'outliers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'for_PCA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zscore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[0;32m    489\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1091\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3229)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6042)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File /home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/Networks/zscore/PCA_means/networks/CP7_DecisionAud_thresh.csv does not exist"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(SJdir,'PCA', 'csvs_FINAL', 'mean_traces_all_subjs_dropSR.csv')\n",
    "df = pd.read_csv(os.path.join(filename))\n",
    "\n",
    "subj_task = df[['subj', 'task']].drop_duplicates()\n",
    "\n",
    "param = 'means'\n",
    "pc = 'PC1'\n",
    "\n",
    "for s_t in subj_task.itertuples():\n",
    "    _, subj, task = s_t\n",
    "\n",
    "    filename = os.path.join(SJdir, 'PCA','Stats', 'Networks', 'zscore', 'PCA_'+param, 'networks', '_'.join([subj, task, 'thresh.csv']))\n",
    "    df_thresh = pd.read_csv(filename)\n",
    "\n",
    "    filename = os.path.join(SJdir, 'PCA','Stats','outliers', 'for_PCA', 'zscore', '_'.join([subj, task, param+'.csv']))\n",
    "    df_data = pd.read_csv(filename)\n",
    "    df_corr = df_data.corr()\n",
    "    \n",
    "    #skip PC2 for subj/task that only have PC1\n",
    "    if not(pc in df_thresh.columns):\n",
    "        continue\n",
    "        \n",
    "    #find elecs that belong to PC group\n",
    "    idx = ~np.isnan(df_thresh[pc])\n",
    "    a = df_thresh[idx].elec\n",
    "    \n",
    "    #index into correlation matrix\n",
    "    a = [str(x) for x in a]\n",
    "    df_corr = df_corr.loc[a,a]\n",
    "    \n",
    "    #save df_corr\n",
    "    filename = os.path.join(SJdir, 'PCA', 'Stats', 'Networks', 'zscore', 'PCA_'+param, 'networks', 'thresholded_correlations', '%s_%s_%s.csv' %(subj, task, pc))\n",
    "    df_corr.to_csv(filename)\n",
    "    \n",
    "    #plot\n",
    "    f, ax = plt.subplots()\n",
    "\n",
    "    df_thresh[idx][['elec', 'pattern', 'ROI']]\n",
    "    b = zip(df_thresh[idx].elec, df_thresh[idx].pattern, df_thresh[idx].ROI)\n",
    "    b = ['%i - %s - %s' %(x) for x in b]\n",
    "\n",
    "    plt.imshow(df_corr, vmin = -1, vmax = 1, interpolation = 'none', cmap = plt.cm.RdBu_r)\n",
    "    ax.set_xticks(range(len(a)))\n",
    "    ax.set_yticks(range(len(a)))\n",
    "    ax.set_xticklabels(a)\n",
    "    ax.set_yticklabels(b)\n",
    "    plt.colorbar()\n",
    "    ax.set_title('%s %s - %s - %s' %(subj, task, param, pc))\n",
    "    \n",
    "    filename = os.path.join(SJdir, 'PCA', 'Stats', 'Networks', 'zscore', 'PCA_'+param, 'networks', 'thresholded_correlations', '%s_%s_%s.png' %(subj, task, pc))\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## update PC csv with revised electrode designations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(SJdir, 'PCA', 'csvs_FINAL', 'mean_traces_all_subjs_dropSR.csv'))\n",
    "df_pca = pd.read_csv(os.path.join(SJdir, 'PCA', 'Stats', 'Networks', 'unsmoothed', 'PCA_medians', 'networks', 'all_subjects_thresh_medians_withclusters.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pca_new = pd.merge(df[['subj','task','elec','pattern']], df_pca[['subj','task','elec','cluster','max_pc','pc_list','ROI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(SJdir,'PCA', 'csvs_FINAL', 'all_subjects_thresh_medians_withclusters_newdesignations.csv')\n",
    "df_pca_new.to_csv(filename)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_pca_new2 = pd.merge(df_pca[['subj','task','elec','cluster','max_pc','pc_list','ROI']], df[['subj','task','elec','pattern']])\n",
    "df_pca_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate proportion explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(SJdir,'PCA', 'csvs_FINAL', 'mean_traces_all_subjs_dropSR.csv')\n",
    "df = pd.read_csv(os.path.join(filename))\n",
    "\n",
    "subj_task = df[['subj', 'task']].drop_duplicates()\n",
    "\n",
    "pc1 = []\n",
    "\n",
    "for s_t in subj_task.itertuples():\n",
    "    _, subj, task = s_t\n",
    "\n",
    "    filename = os.path.join(SJdir, 'PCA','Stats', 'Networks','zscore','PCA_means','%s_%s_summary.txt' %(subj, task))\n",
    "    list_of_lists = []\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            inner_list = [elt.strip() for elt in line.split(',')]\n",
    "            # in alternative, if you need to use the file content as numbers\n",
    "            # inner_list = [int(elt.strip()) for elt in line.split(',')]\n",
    "            list_of_lists.append(inner_list)\n",
    "\n",
    "        tmp = [x[0].split(' ') for x in list_of_lists if x[0].split(' ')[0:2] == ['Proportion', 'Var']][0]\n",
    "        tmp = [t for t in tmp if len(t)>0]\n",
    "        pc1.append(np.float(tmp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2481081081081081"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018469347375263259"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.sem(pc1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
