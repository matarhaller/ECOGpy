{
 "metadata": {
  "name": "",
  "signature": "sha256:b2b66b5918a644dcb4f726ba3fe5240d90dac2601e23e97628ad1e5ddab12ca8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
       ]
      }
     ],
     "prompt_number": 387
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: Qt4Agg\n"
       ]
      }
     ],
     "prompt_number": 388
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os\n",
      "from __future__ import division\n",
      "import numpy as np\n",
      "import cPickle as pickle\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import loadmat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 389
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## make csv files \n",
      "* in ShadePlots_hclust/elecs/significance_windows/csv_files for values calculated within signficance windows\n",
      "* after dropping bad electrodes and NA trials\n",
      "* edited for unsmoothed data\n",
      "* edit for surrogate data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 390
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir,'PCA', 'Stats','single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df = pd.read_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 391
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bad_df = pd.DataFrame({'GP44_DecisionAud':233, 'GP15_SelfVis':1, 'JH2_FaceEmo':113, 'GP35_FaceEmo':60}, index = range(1)).T\n",
      "bad_df = bad_df.reset_index()\n",
      "bad_df.columns = ['subj_task','elec']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 392
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FOR REAL DATA\n",
      "#features = ['lats','lats_pro','maxes','maxes_rel','means','medians','RTs','stds','sums','variations', 'mins', 'lats_min']\n",
      "\n",
      "features = ['maxes','maxes_rel','means','medians','RTs','stds','sums','variations', 'mins']\n",
      "\n",
      "\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    for f in features:\n",
      "        \n",
      "        #load csv file\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files','orig', '_'.join([subj, task, f+'.csv']))\n",
      "        data = pd.read_csv(filename)\n",
      "        \n",
      "        #drop specific elecs\n",
      "        if '_'.join([subj, task]) in bad_df.subj_task.values:\n",
      "            elec_to_drop = bad_df[bad_df.subj_task == '_'.join([subj, task])].elec\n",
      "            data.pop(str(elec_to_drop.values[0]))\n",
      "            \n",
      "        #drop NA trials\n",
      "        data = data.dropna() \n",
      "    \n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "        data.to_csv(filename, index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FOR RANDOM SURROGATE DATA\n",
      "id_num = 99 #for surrogate\n",
      "\n",
      "features = ['maxes_rel','medians', 'RTs', 'maxes_idx', 'medians_idx', 'maxes', 'means', 'stds', 'mins']\n",
      "\n",
      "if os.path.exists(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files','surr_rand_' + str(id_num))):\n",
      "    print '%s\\nexists' %(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_rand_' + str(id_num)))\n",
      "else:\n",
      "    os.mkdir(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_rand_' + str(id_num)))\n",
      "    print 'made %s' %(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_rand_' + str(id_num)))\n",
      "\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    for f in features:\n",
      "        \n",
      "        #load csv file\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files','orig', 'surr_rand_' + str(id_num), '_'.join([subj, task, f+'_surr_rand.csv']))\n",
      "        data = pd.read_csv(filename)\n",
      "        \n",
      "        #drop specific elecs\n",
      "        if '_'.join([subj, task]) in bad_df.subj_task.values:\n",
      "            elec_to_drop = bad_df[bad_df.subj_task == '_'.join([subj, task])].elec\n",
      "            data.pop(str(elec_to_drop.values[0]))\n",
      "            \n",
      "        #drop NA trials\n",
      "        data = data.dropna() \n",
      "    \n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_rand_' + str(id_num), '_'.join([subj, task, f]) + '_surr_rand.csv')\n",
      "        data.to_csv(filename, index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/ShadePlots_hclust/elecs/significance_windows/unsmoothed/csv_files/surr_rand_99\n",
        "exists\n"
       ]
      }
     ],
     "prompt_number": 369
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FOR SURROGATE DATA\n",
      "id_num = 99 #for surrogate\n",
      "\n",
      "features = ['maxes_rel','medians', 'RTs', 'maxes_idx', 'medians_idx', 'maxes', 'means', 'stds', 'mins']\n",
      "\n",
      "if os.path.exists(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files','surr_'+str(id_num))):\n",
      "    print '%s\\nexists' %(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_'+str(id_num)))\n",
      "else:\n",
      "    os.mkdir(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_'+str(id_num)))\n",
      "    print 'made %s' %(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_'+str(id_num)))\n",
      "\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    for f in features:\n",
      "        \n",
      "        #load csv file\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files','orig', 'surr_'+str(id_num), '_'.join([subj, task, f+'_surr.csv']))\n",
      "        data = pd.read_csv(filename)\n",
      "        \n",
      "        #drop specific elecs\n",
      "        if '_'.join([subj, task]) in bad_df.subj_task.values:\n",
      "            elec_to_drop = bad_df[bad_df.subj_task == '_'.join([subj, task])].elec\n",
      "            data.pop(str(elec_to_drop.values[0]))\n",
      "            \n",
      "        #drop NA trials\n",
      "        data = data.dropna() \n",
      "    \n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_'+str(id_num), '_'.join([subj, task, f]) + '_surr.csv')\n",
      "        data.to_csv(filename, index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/ShadePlots_hclust/elecs/significance_windows/unsmoothed/csv_files/surr_99\n",
        "exists\n"
       ]
      }
     ],
     "prompt_number": 393
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for circshifted data\n",
      "features = ['maxes_rel','medians', 'RTs', 'maxes', 'means', 'stds', 'RTs', 'mins']\n",
      "\n",
      "if os.path.exists(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'circshift')):\n",
      "    print '%s\\nexists' %(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'circshift'))\n",
      "else:\n",
      "    os.mkdir(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'circshift'))\n",
      "    print 'made %s' %(os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'circshift'))\n",
      "\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    for f in features:\n",
      "        \n",
      "        #load csv file\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files','orig', 'circshift', '_'.join([subj, task, f+'_circshift.csv']))\n",
      "        data = pd.read_csv(filename)\n",
      "        \n",
      "        #drop specific elecs\n",
      "        if '_'.join([subj, task]) in bad_df.subj_task.values:\n",
      "            elec_to_drop = bad_df[bad_df.subj_task == '_'.join([subj, task])].elec\n",
      "            data.pop(str(elec_to_drop.values[0]))\n",
      "            \n",
      "        #drop NA trials\n",
      "        data = data.dropna() \n",
      "    \n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'circshift', '_'.join([subj, task, f]) + '_circshift.csv')\n",
      "        data.to_csv(filename, index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/ShadePlots_hclust/elecs/significance_windows/unsmoothed/csv_files/circshift\n",
        "exists\n"
       ]
      }
     ],
     "prompt_number": 1232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp = np.random.permutation(10)\n",
      "i = np.argpartition(tmp, -2)[-2:] #find the indices of the 2 largest numbers\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 372
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##combining mean trace values csv into one for all subjects\n",
      "* from smoothed mean data (maxes and latencies). medians/means from single trials\n",
      "* either stim or resp locked\n",
      "* add ROI"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_all = pd.DataFrame()\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "\n",
      "    #load csv file\n",
      "    #filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'smoothed','mean_traces','csv_files', '_'.join([subj, task+'.csv']))\n",
      "    filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'smoothed','mean_traces','csv_files', '_'.join([subj, task, 'resplocked.csv']))\n",
      "    \n",
      "    data = pd.read_csv(filename)\n",
      "    cols = data.columns\n",
      "    cols = ['elec'] + list(cols[1:])\n",
      "    data.columns = cols\n",
      "    data['subj'] = subj\n",
      "    data['task']= task\n",
      "    cols = ['subj','task'] + list(cols[:-2])\n",
      "    data = data[cols]\n",
      "\n",
      "    #drop specific elecs\n",
      "    if '_'.join([subj, task]) in bad_df.subj_task.values:\n",
      "        elec_to_drop = bad_df[bad_df.subj_task == '_'.join([subj, task])].elec\n",
      "        data = data[data.elec != elec_to_drop.values[0]]\n",
      "\n",
      "    df_all = df_all.append(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 328
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#add pattern\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats','single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "df_all = pd.merge(df[['subj','task','elec','pattern','cluster', 'start_idx', 'end_idx', 'start_idx_resp', 'end_idx_resp']], df_all)\n",
      "df_all = df_all.sort(['subj','task', 'elec'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 329
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#add ROI\n",
      "def get_ROI(subj, e, brain_ROI, task = None):\n",
      "    '''\n",
      "    Returns an ROI for a given subj and elec.\n",
      "    If subject is GP35 then must give a task argument so knows which ROIs to pull\n",
      "    '''\n",
      "    \n",
      "    if (subj == 'GP35') & ((task == 'EmoRep') | (task == 'EmoGen')):\n",
      "        subj = 'GP35_words'\n",
      "    elif (subj == 'GP35') & ((task == 'FaceEmo') | (task == 'FaceGen')):\n",
      "        subj = 'GP35_face'\n",
      "    \n",
      "    try:\n",
      "        rois = brain_ROI[subj]\n",
      "    except:\n",
      "        return None\n",
      "    else:\n",
      "        for roi, elecs in rois.iteritems():\n",
      "            if np.in1d(e, elecs):\n",
      "                return roi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 330
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = 'brain_ROI.mat'\n",
      "filename = os.path.join(SJdir, 'ROIs', 'brain_ROI.mat')\n",
      "data = loadmat.loadmat(filename)\n",
      "brain_ROI = data['brain_ROI']\n",
      "\n",
      "df_all['ROI'] = df_all.apply(lambda x:get_ROI(x.subj, x.elec, brain_ROI, task = x.task), axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 331
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'smoothed', 'mean_traces', 'csv_files', 'mean_traces_all_subjs.csv')\n",
      "filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'smoothed', 'mean_traces', 'csv_files', 'mean_traces_all_subjs_resplocked.csv')\n",
      "df_all.to_csv(filename, index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 332
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##calculating dropped trials\n",
      "* for each duration electrode - proportion/number of trials with <100 ms gamma duration from gamma onset to window offset\n",
      "* calculate for each duration elec how many (percentage/number) of trials have \"active window\" shorter than 100 ms?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'\n",
      "#filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "\n",
      "df = pd.read_csv(filename)\n",
      "f = 'RTs'\n",
      "subjs, tasks, elecs, total100, total500, proportion100, proportion500, minimum, starts, ends = [[] for i in range(10)]\n",
      "\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "\n",
      "    #load data\n",
      "    filename = os.path.join(SJdir, 'Subjs', subj, task, 'HG_elecMTX_percent.mat')\n",
      "    data_dict = loadmat.loadmat(filename)\n",
      "    srate, data_percent, active_elecs = [data_dict.get(k) for k in ['srate', 'data_percent', 'active_elecs']]\n",
      "    \n",
      "    bl_st = -500/1000*srate\n",
      "    \n",
      "    #load RTs csv file (need to remove bl from the RTs)   \n",
      "    filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows','unsmoothed', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "    data = pd.read_csv(filename)\n",
      "    RTs = np.array(data)[:,0]\n",
      "    RTs = RTs-abs(bl_st)\n",
      "    \n",
      "    df_subj = df[(df.subj == subj) & (df.task == task) & (df.pattern == 'D')][['elec','start_idx','end_idx_resp']]    \n",
      "    \n",
      "    for row in df_subj.itertuples():\n",
      "        _, elec, start_idx, end_idx_resp = row\n",
      "        \n",
      "        total500.append(sum(((RTs+end_idx_resp)-start_idx)<500))\n",
      "        proportion500.append(total500[-1]/len(RTs))\n",
      "        total100.append(sum(((RTs+end_idx_resp)-start_idx)<100))\n",
      "        proportion100.append(total100[-1]/len(RTs))\n",
      "        minimum.append(np.min(RTs+end_idx_resp)-start_idx)\n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        elecs.append(elec)\n",
      "        starts.append(start_idx)\n",
      "        ends.append(end_idx_resp)\n",
      "        \n",
      "df_stats = pd.DataFrame({'subj':subjs,'task':tasks,'elec':elecs,'total100':total100, 'proportion100':proportion100, 'total500':total500, 'proportion500':proportion500, 'minimum':minimum, 'start_idx':starts, 'end_idx_resp':ends})\n",
      "df_stats = df_stats[['subj','task','elec','start_idx','end_idx_resp', 'total100','proportion100','total500', 'proportion500', 'minimum']]\n",
      "df_stats.columns = ['subj','task','elec','start_idx','end_idx_resp', 'total100','proportion100', 'total500', 'proportion500', 'min window']\n",
      "\n",
      "#SAVE\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'duration_elec_shortwindow.csv')\n",
      "df_stats.to_csv(filename)\n",
      "## load in values calcuated in ShadePlots_elecs_stats_staticwindow.py for each electrode"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1197
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## calculate difference between latencies (and maxes) with static and with original (RT-dependent) window\n",
      "* mean value of latency for each electrode"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "bad_df = pd.DataFrame({'GP44_DecisionAud':233, 'GP15_SelfVis':1, 'JH2_FaceEmo':113, 'GP35_FaceEmo':60}, index = range(1)).T\n",
      "bad_df = bad_df.reset_index()\n",
      "bad_df.columns = ['subj_task','elec']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df for latencies\n",
      "\n",
      "df_total = pd.DataFrame(columns = ['subj','task','elec','orig_lat','static_lat'])\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA','ShadePlots_hclust','elecs','significance_windows','csv_files', '_'.join([subj, task, 'lats.csv']))\n",
      "    dforig = pd.read_csv(filename)\n",
      "    filename = os.path.join(SJdir, 'PCA','ShadePlots_hclust','elecs','significance_windows','static','csv_files', '_'.join([subj, task, 'lats.csv']))\n",
      "    dfstatic = pd.read_csv(filename)\n",
      "\n",
      "    df_all = pd.concat([dforig.mean(axis =0),dfstatic.mean(axis = 0)], axis = 1).reset_index()\n",
      "    df_all.columns = ['elec','orig_lat','static_lat']\n",
      "    df_all['subj'] = subj\n",
      "    df_all['task'] = task\n",
      "\n",
      "    df_total = df_total.append(df_all)\n",
      "\n",
      "df_total2 = pd.DataFrame(columns = ['subj','task','elec', 'orig_max','static_max'])\n",
      "for s_t in df.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA','ShadePlots_hclust','elecs','significance_windows','csv_files', '_'.join([subj, task, 'maxes_rel.csv']))\n",
      "    dforig = pd.read_csv(filename)\n",
      "    filename = os.path.join(SJdir, 'PCA','ShadePlots_hclust','elecs','significance_windows','static','csv_files', '_'.join([subj, task, 'maxes_rel.csv']))\n",
      "    dfstatic = pd.read_csv(filename)\n",
      "\n",
      "    df_all = pd.concat([dforig.mean(axis =0),dfstatic.mean(axis = 0)], axis = 1).reset_index()\n",
      "    df_all.columns = ['elec','orig_max','static_max']\n",
      "    df_all['subj'] = subj\n",
      "    df_all['task'] = task\n",
      "    df_total2 = df_total2.append(df_all)\n",
      "\n",
      "    \n",
      "df_full = df_total.merge(df_total2)\n",
      "df_full = df_full[['subj','task','elec','orig_lat','static_lat', 'orig_max','static_max']]\n",
      "\n",
      "df_full.elec = df_full.elec.astype(int)\n",
      "\n",
      "df_full = pd.merge(df_full, df[['subj','task','elec','pattern']])\n",
      "\n",
      "filename = os.path.join(SJdir, 'PCA','Stats', 'latencies', 'static', 'orig_vs_static_comparison.csv')\n",
      "df_full.to_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## calculate number of outlier trials at +/- 4 stds (not used)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = ['RTs','maxes','means','lats_pro','stds', 'maxes_rel','medians']\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df_all = pd.read_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for s_t in df_all.groupby(['subj','task']):\n",
      "    subj, task = s_t[0]\n",
      "    df_out = pd.DataFrame()\n",
      "    df_out_trials = pd.DataFrame()\n",
      "    \n",
      "    #load data\n",
      "    for f in features:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "        \n",
      "        tmp1 = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*4)) | (x < (x.mean() - x.std(ddof = 1)* 4))))\n",
      "        tmp2 = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*3.5)) | (x < (x.mean() - x.std(ddof = 1)* 3.5))))\n",
      "        \n",
      "        df_out[f+'_4'] = tmp1.sum()\n",
      "        df_out[f+'_3.5'] = tmp2.sum()\n",
      "        \n",
      "        df_out_trials[f+'_4'] = tmp1.sum(axis = 1)\n",
      "        df_out_trials[f+'_3.5'] = tmp2.sum(axis = 1)\n",
      "        \n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers', 'stats', '_'.join([subj, task]) + '.csv')\n",
      "    df_out.to_csv(filename)\n",
      "\n",
      "    filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','stats', '_'.join([subj, task]) + '_trials.csv')\n",
      "    df_out_trials.to_csv(filename)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 590
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# DROP OUTLIER TRIALS \n",
      "####outliers only calculated based on on stds, relative max, and median - not on latencies or RTs (RT outliers already removed)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs','single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df_all = pd.read_csv(filename)\n",
      "std_thresh = 4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PCA outlier rejection\n",
      "* all electrodes have the same number of trials\n",
      "* each feature can have a different number of trials\n",
      "* save separate file for each feature, plus an RT file for each feature to match number of trials dropped\n",
      "* for unsmoothed data\n",
      "* edited for surrogates"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#outlier rejection of RTs to match outlier rejection of PCA for each param\n",
      "features_to_consider = ['means', 'medians', 'maxes', 'maxes_rel', 'stds']\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list = [[] for i in range(5)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "        \n",
      "    subj, task = s_t[0]\n",
      "\n",
      "    #find outlier trials\n",
      "    for f in features_to_consider:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows','unsmoothed', 'csv_files', '_'.join([subj, task, 'RTs']) + '.csv')\n",
      "        df_RT = pd.read_csv(filename)\n",
      "\n",
      "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "\n",
      "        outliers = np.where(trials_to_drop)[0]\n",
      "\n",
      "        #drop outliers\n",
      "        df.iloc[outliers] = np.nan\n",
      "        df = df.dropna()\n",
      "\n",
      "        df_RT.iloc[outliers] = np.nan\n",
      "        df_RT = df_RT.dropna()\n",
      "\n",
      "        filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_PCA', 'unsmoothed', '_'.join([subj, task, f + '.csv']))\n",
      "        df.to_csv(filename, index = False)\n",
      "        filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_PCA', 'unsmoothed', '_'.join([subj, task, f + '_RTs.csv'])) #RTs per elec (identical) with correct number of trials\n",
      "        df_RT.to_csv(filename, index = False)\n",
      "\n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        num_total.append(trials_to_drop.shape[0])\n",
      "        num_to_drop.append(len(outliers))\n",
      "        feature_list.append(f)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#make a summary\n",
      "df_tally = pd.DataFrame({'subj':subjs, 'task': tasks, 'total' : num_total, 'outliers' : num_to_drop, 'feature':feature_list})\n",
      "df_tally['percent_dropped'] = df_tally['outliers']/df_tally['total'] * 100\n",
      "df_tally = df_tally[['subj','task','feature', 'outliers','total','percent_dropped']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_tally.iloc[np.argmax(df_tally.percent_dropped)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 164,
       "text": [
        "subj                      GP44\n",
        "task               DecisionAud\n",
        "feature              maxes_rel\n",
        "outliers                    82\n",
        "total                      216\n",
        "percent_dropped       37.96296\n",
        "Name: 83, dtype: object"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers', 'for_PCA','unsmoothed', 'tally_all.csv')\n",
      "df_tally.to_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 165
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regression outlier rejection\n",
      "* all features have the same number of trials (across electrodes)\n",
      "* each electrode can have a different number of trials\n",
      "* for unsmoothed data\n",
      "* relative max and median run separately from stds"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df_all = pd.read_csv(filename)\n",
      "std_thresh = 4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 394
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##REAL DATA\n",
      "folder = 'stds_only'\n",
      "features_to_consider = ['stds']\n",
      "\n",
      "\n",
      "saveDir_data= os.path.join(SJdir,'PCA','Stats', 'outliers','for_Regression', 'unsmoothed', folder)\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs')\n",
      "\n",
      "#os.mkdir(saveDir_data)\n",
      "#os.mkdir(saveDir_csv)\n",
      "print('made:\\n%s\\nand\\n%s' %(saveDir_csv, saveDir_data))\n",
      "\n",
      "predictor = 'RTs'\n",
      "features_plus_predictor = features_to_consider + [predictor]\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "\n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files',  '_'.join([subj, task, predictor]))\n",
      "    df_RT = pd.read_csv(filename + '.csv')\n",
      "    df_RT.columns = [int(x) for x in df_RT.columns]\n",
      "    df_dict['RTs'] = df_RT\n",
      "    \n",
      "    #find outlier trials\n",
      "    for f in features_to_consider:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', '_'.join([subj, task, f]))\n",
      "        df = pd.read_csv(filename + '.csv')\n",
      "        \n",
      "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "        \n",
      "        masked_values = np.where(~trials_to_drop, df.values, np.nan) #good trials\n",
      "              \n",
      "        tmp = pd.DataFrame(masked_values, **df._construct_axes_dict()) #new dataframe with dropped trials for 1 feature\n",
      "        tmp.columns = [int(x) for x in tmp.columns]\n",
      "        df_dict[f] = tmp\n",
      "        \n",
      "    #drop nans for all features for an electrode    \n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_plus_predictor]) #features x trials\n",
      "\n",
      "        trials_to_drop = np.any(np.isnan(data_array), axis = 0)\n",
      "        \n",
      "        if np.any(trials_to_drop):\n",
      "            tmp = [features_plus_predictor[i] for i in np.where(np.isnan(data_array))[0]]\n",
      "            feature_list.append(tmp)\n",
      "        else:\n",
      "            feature_list.append('')\n",
      "            \n",
      "        data_array = data_array[:,~trials_to_drop]\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_plus_predictor)\n",
      "        \n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        data_dict[elec].to_csv(filename + '.csv', index = False)\n",
      "        \n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        num_total.append(trials_to_drop.shape[0])\n",
      "        num_to_drop.append(sum(trials_to_drop))\n",
      "        all_elecs.append(elec)\n",
      "    \n",
      "    filename = os.path.join(saveDir_data, '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "made:\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/stds_only/elec_csvs\n",
        "and\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/stds_only\n"
       ]
      }
     ],
     "prompt_number": 280
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##RANDOM SURROGATE DATA \n",
      "#(if for INDICES- drop outliers based on non-idices surrogate data\n",
      "id_num = 99\n",
      "datafolder = 'maxes_medians'\n",
      "savefolder = 'maxes_medians'\n",
      "features_to_consider = ['maxes_rel', 'medians']\n",
      "predictor = 'RTs'\n",
      "features_plus_predictor = features_to_consider + [predictor]\n",
      "\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'elec_csvs', 'surr_rand_' + str(id_num))\n",
      "saveDir_data= os.path.join(SJdir,'PCA','Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'surr_rand_' + str(id_num))\n",
      "\n",
      "#os.mkdir(saveDir_csv)\n",
      "#os.mkdir(saveDir_data)\n",
      "print('made\\n%s\\nand\\n%s' %(saveDir_csv, saveDir_data))\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "\n",
      "    subj, task = s_t[0]\n",
      "        \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_rand_' + str(id_num), '_'.join([subj, task, predictor]) + '_surr_rand.csv')\n",
      "    df_RT = pd.read_csv(filename)\n",
      "    df_RT.columns = [int(x) for x in df_RT.columns]\n",
      "    df_dict['RTs'] = df_RT\n",
      "   \n",
      "    #find outlier trials\n",
      "    for f in features_to_consider:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_rand_' + str(id_num), '_'.join([subj, task, f]) + '_surr_rand.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "        \n",
      "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "        \n",
      "        masked_values = np.where(~trials_to_drop, df.values, np.nan) #good trials in data_idx (defined by data_surr)     \n",
      "        tmp = pd.DataFrame(masked_values, **df._construct_axes_dict()) #new dataframe with dropped trials for 1 feat\n",
      "\n",
      "        tmp.columns = [int(x) for x in tmp.columns]\n",
      "        df_dict[f] = tmp\n",
      "        \n",
      "    #drop nans for all features for an electrode    \n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_plus_predictor]) #features x trials\n",
      "\n",
      "        trials_to_drop = np.any(np.isnan(data_array), axis = 0)\n",
      "        \n",
      "        if np.any(trials_to_drop):\n",
      "            tmp = [features_plus_predictor[i] for i in np.where(np.isnan(data_array))[0]]\n",
      "            feature_list.append(tmp)\n",
      "        else:\n",
      "            feature_list.append('')\n",
      "            \n",
      "        data_array = data_array[:,~trials_to_drop]\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_plus_predictor)\n",
      "        \n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        data_dict[elec].to_csv(filename + '_surr_rand.csv', index = False)\n",
      "        \n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        num_total.append(trials_to_drop.shape[0])\n",
      "        num_to_drop.append(sum(trials_to_drop))\n",
      "        all_elecs.append(elec)\n",
      "    \n",
      "    filename = os.path.join(saveDir_data, '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '_surr_rand.p', 'wb'))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "made\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/elec_csvs/surr_rand_99\n",
        "and\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/surr_rand_99\n"
       ]
      }
     ],
     "prompt_number": 382
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data_array[0].shape\n",
      "print df_RT.shape\n",
      "df.shape\n",
      "print (subj, task, elec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(73,)\n",
        "(74, 26)\n",
        "('GP28', 'SelfAud', 3)\n"
       ]
      }
     ],
     "prompt_number": 363
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##SURROGATE DATA \n",
      "#(if for INDICES- drop outliers based on non-idices surrogate data\n",
      "id_num = 99\n",
      "datafolder = 'maxes_medians'\n",
      "savefolder = 'maxes_medians'\n",
      "features_to_consider = ['maxes_rel', 'medians']\n",
      "predictor = 'RTs'\n",
      "features_plus_predictor = features_to_consider + [predictor]\n",
      "\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'elec_csvs', 'surr_' + str(id_num))\n",
      "saveDir_data= os.path.join(SJdir,'PCA','Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'surr_' + str(id_num))\n",
      "\n",
      "os.mkdir(saveDir_csv)\n",
      "os.mkdir(saveDir_data)\n",
      "print('made\\n%s\\nand\\n%s' %(saveDir_csv, saveDir_data))\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "\n",
      "    subj, task = s_t[0]\n",
      "        \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_' + str(id_num), '_'.join([subj, task, predictor]) + '_surr.csv')\n",
      "    df_RT = pd.read_csv(filename)\n",
      "    df_RT.columns = [int(x) for x in df_RT.columns]\n",
      "    df_dict['RTs'] = df_RT\n",
      "   \n",
      "    #find outlier trials\n",
      "    for f in features_to_consider:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_' + str(id_num), '_'.join([subj, task, f]) + '_surr.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "        \n",
      "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "        \n",
      "        masked_values = np.where(~trials_to_drop, df.values, np.nan) #good trials in data_idx (defined by data_surr)     \n",
      "        tmp = pd.DataFrame(masked_values, **df._construct_axes_dict()) #new dataframe with dropped trials for 1 feat\n",
      "\n",
      "        tmp.columns = [int(x) for x in tmp.columns]\n",
      "        df_dict[f] = tmp\n",
      "        \n",
      "    #drop nans for all features for an electrode    \n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_plus_predictor]) #features x trials\n",
      "\n",
      "        trials_to_drop = np.any(np.isnan(data_array), axis = 0)\n",
      "        \n",
      "        if np.any(trials_to_drop):\n",
      "            tmp = [features_plus_predictor[i] for i in np.where(np.isnan(data_array))[0]]\n",
      "            feature_list.append(tmp)\n",
      "        else:\n",
      "            feature_list.append('')\n",
      "            \n",
      "        data_array = data_array[:,~trials_to_drop]\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_plus_predictor)\n",
      "        \n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        data_dict[elec].to_csv(filename + '_surr.csv', index = False)\n",
      "        \n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        num_total.append(trials_to_drop.shape[0])\n",
      "        num_to_drop.append(sum(trials_to_drop))\n",
      "        all_elecs.append(elec)\n",
      "    \n",
      "    filename = os.path.join(saveDir_data, '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '_surr.p', 'wb'))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "made\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/elec_csvs/surr_99\n",
        "and\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/surr_99\n"
       ]
      }
     ],
     "prompt_number": 395
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##CIRCSHIFTED DATA\n",
      "datafolder = 'stds_only'\n",
      "savefolder = 'stds_only'\n",
      "\n",
      "features_to_consider = ['stds']\n",
      "predictor = 'RTs'\n",
      "features_plus_predictor = features_to_consider + [predictor]\n",
      "\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'elec_csvs', 'circshift')\n",
      "saveDir_data = os.path.join(SJdir,'PCA','Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'circshift')\n",
      "\n",
      "os.mkdir(saveDir_csv)\n",
      "os.mkdir(saveDir_data)\n",
      "print('made\\n%s\\nand\\n%s' %(saveDir_csv, saveDir_data))\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "\n",
      "    subj, task = s_t[0]\n",
      "        \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'circshift', '_'.join([subj, task, predictor]) + '_circshift.csv')\n",
      "    df_RT = pd.read_csv(filename)\n",
      "    df_RT.columns = [int(x) for x in df_RT.columns]\n",
      "    df_dict['RTs'] = df_RT\n",
      "   \n",
      "    #find outlier trials\n",
      "    for f in features_to_consider:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'circshift', '_'.join([subj, task, f]) + '_circshift.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "                \n",
      "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "        \n",
      "        masked_values = np.where(~trials_to_drop, df.values, np.nan) #good trials in data_idx (defined by data_surr)     \n",
      "        tmp = pd.DataFrame(masked_values, **df._construct_axes_dict()) #new dataframe with dropped trials for 1 feat\n",
      "        \n",
      "        tmp.columns = [int(x) for x in tmp.columns]\n",
      "        df_dict[f] = tmp\n",
      "        \n",
      "    #drop nans for all features for an electrode    \n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_plus_predictor]) #features x trials\n",
      "\n",
      "        trials_to_drop = np.any(np.isnan(data_array), axis = 0)\n",
      "        \n",
      "        if np.any(trials_to_drop):\n",
      "            tmp = [features_plus_predictor[i] for i in np.where(np.isnan(data_array))[0]]\n",
      "            feature_list.append(tmp)\n",
      "        else:\n",
      "            feature_list.append('')\n",
      "            \n",
      "        data_array = data_array[:,~trials_to_drop]\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_plus_predictor)\n",
      "        \n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        data_dict[elec].to_csv(filename + '_circshift.csv', index = False)\n",
      "        \n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        num_total.append(trials_to_drop.shape[0])\n",
      "        num_to_drop.append(sum(trials_to_drop))\n",
      "        all_elecs.append(elec)    \n",
      "    \n",
      "    filename = os.path.join(saveDir_data, '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '_circshift.p', 'wb'))\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "made\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/stds_only/elec_csvs/circshift\n",
        "and\n",
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/stds_only/circshift\n"
       ]
      }
     ],
     "prompt_number": 285
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#make a summary\n",
      "df_tally = pd.DataFrame({'subj':subjs, 'task': tasks, 'total' : num_total, 'outliers' : num_to_drop, 'elec': all_elecs, 'outlier features':feature_list})\n",
      "df_tally['percent_dropped'] = df_tally['outliers']/df_tally['total'] * 100\n",
      "df_tally = df_tally[['subj','task','elec', 'outlier features', 'outliers','total','percent_dropped']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 396
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print saveDir_csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/elec_csvs/surr_99\n"
       ]
      }
     ],
     "prompt_number": 397
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(saveDir_csv, 'tally_all_surr.csv')\n",
      "df_tally.to_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 398
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_tally.loc[np.argmax(df_tally.percent_dropped)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 399,
       "text": [
        "subj                                  JH10\n",
        "task                               SelfAud\n",
        "elec                                     9\n",
        "outlier features    [maxes_rel, maxes_rel]\n",
        "outliers                                 2\n",
        "total                                   59\n",
        "percent_dropped                   3.389831\n",
        "Name: 530, dtype: object"
       ]
      }
     ],
     "prompt_number": 399
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Drop HG windows <200ms\n",
      "* each electrode can have a different number of trials (but drop bad trial across all features)\n",
      "* for unsmoothed data\n",
      "* edited for surrogate data\n",
      "* can run with or without std"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bad_df = pd.DataFrame({'GP44_DecisionAud':233, 'GP15_SelfVis':1, 'JH2_FaceEmo':113, 'GP35_FaceEmo':60}, index = range(1)).T\n",
      "bad_df = bad_df.reset_index()\n",
      "bad_df.columns = ['subj_task','elec']\n",
      "\n",
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 400
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#REAL DATA\n",
      "folder = 'stds_only'\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs')\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows')\n",
      "\n",
      "os.mkdir(saveDir_csv)\n",
      "\n",
      "for row in df.itertuples():\n",
      "    trial_lengths = []\n",
      "    i, subj, task, cluster, pattern, elec, start_idx, end_idx, start_idx_resp, end_idx_resp, _, _ = row\n",
      "    #load data\n",
      "    filename = os.path.join(SJdir, 'Subjs', subj, task, 'subj_globals.mat')\n",
      "    data_dict = loadmat.loadmat(filename)\n",
      "    srate = [data_dict.get(k) for k in ['srate']][0]\n",
      "    srate = float(srate)\n",
      "    min_window = round(200/1000*srate) #rounding\n",
      "    bl_st = -500/1000*srate #for all subj/tasks (since cue already accounted for in start/end_idx)\n",
      "\n",
      "    #skip specific elecs\n",
      "    if len(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec)>0:\n",
      "        if any(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec):\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "\n",
      "    if ((pattern == 'S') | (pattern == 'SR') | (pattern == 'sustained') | (pattern == 'S+sustained')) & ((end_idx - start_idx) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        df_trialvalues = pd.read_csv(filename + '.csv')\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        df_trialvalues.to_csv(filename + '.csv', index = False)\n",
      "\n",
      "    if (pattern == 'R') & ((end_idx_resp - start_idx_resp) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        df_trialvalues = pd.read_csv(filename + '.csv')\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        df_trialvalues.to_csv(filename + '.csv', index = False)\n",
      "\n",
      "    if (pattern == 'D'):\n",
      "        trial_lengths = []\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        df_trialvalues = pd.read_csv(filename + '.csv')\n",
      "        start_idx = start_idx + abs(bl_st)\n",
      "        RTs = df_trialvalues.RTs\n",
      "\n",
      "        for i, r in enumerate(RTs):\n",
      "            trial_lengths.append(int(r+end_idx_resp-start_idx)) #length of each short trial so can use for long trial indexing\n",
      "\n",
      "        trials_to_drop = np.array(trial_lengths) < min_window\n",
      "        if any(trials_to_drop):\n",
      "            df_trialvalues = df_trialvalues[~trials_to_drop] #drop trials\n",
      "\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)]))\n",
      "        df_trialvalues.to_csv(filename + '.csv', index = False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "skipping GP15 SelfVis 1\n",
        "skipping GP35 FaceEmo 60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 233"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 291
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#REAL DATA\n",
      "#make pickle files based on the csv for use in regression\n",
      "folder = 'stds_only'\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats','single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "\n",
      "saveDir_data = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'no_short_windows')\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows')\n",
      "\n",
      "os.mkdir(saveDir_data)\n",
      "\n",
      "for row in df.groupby(['subj','task']):\n",
      "    subj, task = row[0]\n",
      "    elecs = row[1].elec\n",
      "    df_list = []\n",
      "    \n",
      "    for elec in elecs:\n",
      "\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'.csv']))\n",
      "        if os.path.exists(filename):\n",
      "            df_curr = pd.read_csv(filename)\n",
      "            df_list.append(df_curr)\n",
      "        else:\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "    \n",
      "    data_dict = dict(zip(elecs, df_list))\n",
      "    filename = os.path.join(saveDir_data, '%s_%s.p' %(subj, task))\n",
      "    pickle.dump(data_dict, open(filename, 'wb'))\n",
      "    \n",
      "#all skipped elecs have windows <200 ms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "skipping CP9 DecisionAud 25\n",
        "skipping CP9 DecisionAud 24\n",
        "skipping CP9 DecisionAud 91\n",
        "skipping GP15 FaceGen 24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP15 SelfAud 12\n",
        "skipping GP15 SelfVis 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP27 SelfAud 39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 EmoRep 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceEmo 60\n",
        "skipping GP35 FaceGen 42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceGen 59\n",
        "skipping GP35 FaceGen 60\n",
        "skipping GP44 DecisionAud 233\n",
        "skipping GP44 DecisionAud 19\n",
        "skipping GP44 DecisionAud 101"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 143\n",
        "skipping GP44 DecisionAud 165\n",
        "skipping GP44 DecisionAud 65\n",
        "skipping GP44 DecisionAud 73\n",
        "skipping GP44 DecisionAud 89\n",
        "skipping GP44 DecisionAud 129\n",
        "skipping GP44 DecisionAud 142\n",
        "skipping JH10 SelfAud 11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 EmoRep 39\n",
        "skipping JH17 EmoRep 73\n",
        "skipping JH17 EmoRep 26\n",
        "skipping JH17 SelfAud 99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 SelfVis 68\n",
        "skipping JH17 SelfVis 79\n",
        "skipping JH17 SelfVis 72\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 18\n",
        "skipping JH2 SelfVis 7\n",
        "skipping ST1 EmoGen 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST1 SelfAud 15\n",
        "skipping ST4 EmoRep 59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST4 EmoRep 60\n",
        "skipping ST4 SelfAud 22\n",
        "skipping ST4 SelfAud 58\n",
        "skipping ST4 SelfAud 59\n",
        "skipping ST4 SelfAud 60\n",
        "skipping ST4 SelfAud 19\n",
        "skipping ST6 SelfVis 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 292
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## with SURROGATE RANDOM \n",
      "id_num = 99\n",
      "folder = 'maxes_medians'\n",
      "\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'surr_rand_' + str(id_num))\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows', 'surr_rand_' + str(id_num))\n",
      "\n",
      "if not(os.path.exists(saveDir_csv)):\n",
      "    os.mkdir(saveDir_csv)\n",
      "else:\n",
      "    print(' %s\\n already exists!\\n...' %(saveDir_csv))\n",
      "\n",
      "    \n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "for row in df.itertuples():\n",
      "    i, subj, task, cluster, pattern, elec, start_idx, end_idx, start_idx_resp, end_idx_resp, _, _ = row\n",
      "    #load data\n",
      "    filename = os.path.join(SJdir, 'Subjs', subj, task, 'subj_globals.mat')\n",
      "    data_dict = loadmat.loadmat(filename)\n",
      "    srate = [data_dict.get(k) for k in ['srate']][0]\n",
      "    srate = float(srate)\n",
      "    min_window = round(200/1000*srate)\n",
      "    bl_st = -500/1000*srate #for all subj/tasks (since cue already accounted for in start/end_idx)\n",
      "\n",
      "    #skip specific elecs\n",
      "    if len(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec)>0:\n",
      "        if any(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec):\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "\n",
      "    if ((pattern == 'S') | (pattern == 'SR') | (pattern == 'sustained') | (pattern == 'S+sustained')) & ((end_idx - start_idx) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr_rand.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr_rand.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n",
      "\n",
      "    if (pattern == 'R') & ((end_idx_resp - start_idx_resp) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr_rand.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr_rand.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n",
      "\n",
      "    if (pattern == 'D'):\n",
      "    \n",
      "        trial_lengths = []\n",
      "        \n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr_rand.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        start_idx = start_idx + abs(bl_st)\n",
      "        RTs = df_trialvalues.RTs\n",
      "\n",
      "        for i, r in enumerate(RTs):\n",
      "            trial_lengths.append(int(r+end_idx_resp-start_idx)) #length of each short trial so can use for long trial indexing\n",
      "        trials_to_drop = np.array(trial_lengths) < min_window\n",
      "        \n",
      "        if any(trials_to_drop):\n",
      "            df_trialvalues = df_trialvalues[~trials_to_drop] #drop trials\n",
      "\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr_rand.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " /home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/elec_csvs/no_short_windows/surr_rand_99\n",
        " already exists!\n",
        "...\n",
        "skipping GP15 SelfVis 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceEmo 60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 233"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 384
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#with SURROGATE RANDOM\n",
      "#make pickle files based on the csv for use in regression\n",
      "id_num = 99\n",
      "folder = 'maxes_medians'\n",
      "\n",
      "saveDir_data = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'no_short_windows', 'surr_rand_' + str(id_num))\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows', 'surr_rand_' + str(id_num))\n",
      "\n",
      "if not(os.path.exists(saveDir_data)):\n",
      "    os.mkdir(saveDir_data)\n",
      "else:\n",
      "    print(' %s\\n already exists!...' %(saveDir_data))\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats','single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "for row in df.groupby(['subj','task']):\n",
      "    subj, task = row[0]\n",
      "    elecs = row[1].elec\n",
      "    df_list = []\n",
      "    \n",
      "    for elec in elecs:\n",
      "\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr_rand.csv']))\n",
      "        if os.path.exists(filename):\n",
      "            df_curr = pd.read_csv(filename)\n",
      "            df_list.append(df_curr)\n",
      "        else:\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "    \n",
      "    data_dict = dict(zip(elecs, df_list))\n",
      "    filename = os.path.join(saveDir_data, '%s_%s_surr_rand.p' %(subj, task))\n",
      "    pickle.dump(data_dict, open(filename, 'wb'))\n",
      "    \n",
      "#all skipped elecs have windows <200 ms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " /home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/no_short_windows/surr_rand_99\n",
        " already exists!...\n",
        "skipping CP9 DecisionAud 25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping CP9 DecisionAud 24\n",
        "skipping CP9 DecisionAud 91\n",
        "skipping GP15 FaceGen 24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP15 SelfAud 12\n",
        "skipping GP15 SelfVis 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP27 SelfAud 39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 EmoRep 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceEmo 60\n",
        "skipping GP35 FaceGen 42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceGen 59\n",
        "skipping GP35 FaceGen 60\n",
        "skipping GP44 DecisionAud 233\n",
        "skipping GP44 DecisionAud 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 101\n",
        "skipping GP44 DecisionAud 143\n",
        "skipping GP44 DecisionAud 165\n",
        "skipping GP44 DecisionAud 65\n",
        "skipping GP44 DecisionAud 73\n",
        "skipping GP44 DecisionAud 89\n",
        "skipping GP44 DecisionAud 129"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 142\n",
        "skipping JH10 SelfAud 11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 EmoRep 39\n",
        "skipping JH17 EmoRep 73"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 EmoRep 26\n",
        "skipping JH17 SelfAud 99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 SelfVis 68\n",
        "skipping JH17 SelfVis 79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 SelfVis 72\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 18\n",
        "skipping JH2 SelfVis 7\n",
        "skipping ST1 EmoGen 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST1 SelfAud 15\n",
        "skipping ST4 EmoRep 59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST4 EmoRep 60\n",
        "skipping ST4 SelfAud 22\n",
        "skipping ST4 SelfAud 58\n",
        "skipping ST4 SelfAud 59\n",
        "skipping ST4 SelfAud 60\n",
        "skipping ST4 SelfAud 19\n",
        "skipping ST6 SelfVis 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 386
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## with SURROGATE\n",
      "id_num = 99\n",
      "folder = 'maxes_medians'\n",
      "\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'surr_' + str(id_num))\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows', 'surr_' + str(id_num))\n",
      "\n",
      "if not(os.path.exists(saveDir_csv)):\n",
      "    os.mkdir(saveDir_csv)\n",
      "else:\n",
      "    print(' %s\\n already exists!\\n...' %(saveDir_csv))\n",
      "\n",
      "    \n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "for row in df.itertuples():\n",
      "    i, subj, task, cluster, pattern, elec, start_idx, end_idx, start_idx_resp, end_idx_resp, _, _ = row\n",
      "    #load data\n",
      "    filename = os.path.join(SJdir, 'Subjs', subj, task, 'subj_globals.mat')\n",
      "    data_dict = loadmat.loadmat(filename)\n",
      "    srate = [data_dict.get(k) for k in ['srate']][0]\n",
      "    srate = float(srate)\n",
      "    min_window = round(200/1000*srate)\n",
      "    bl_st = -500/1000*srate #for all subj/tasks (since cue already accounted for in start/end_idx)\n",
      "\n",
      "    #skip specific elecs\n",
      "    if len(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec)>0:\n",
      "        if any(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec):\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "\n",
      "    if ((pattern == 'S') | (pattern == 'SR') | (pattern == 'sustained') | (pattern == 'S+sustained')) & ((end_idx - start_idx) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n",
      "\n",
      "    if (pattern == 'R') & ((end_idx_resp - start_idx_resp) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n",
      "\n",
      "    if (pattern == 'D'):\n",
      "    \n",
      "        trial_lengths = []\n",
      "        \n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        start_idx = start_idx + abs(bl_st)\n",
      "        RTs = df_trialvalues.RTs\n",
      "\n",
      "        for i, r in enumerate(RTs):\n",
      "            trial_lengths.append(int(r+end_idx_resp-start_idx)) #length of each short trial so can use for long trial indexing\n",
      "        trials_to_drop = np.array(trial_lengths) < min_window\n",
      "        \n",
      "        if any(trials_to_drop):\n",
      "            df_trialvalues = df_trialvalues[~trials_to_drop] #drop trials\n",
      "\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " /home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outliers/for_Regression/unsmoothed/maxes_medians/elec_csvs/no_short_windows/surr_99\n",
        " already exists!\n",
        "...\n",
        "skipping GP15 SelfVis 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceEmo 60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 233"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 401
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#with SURROGATE\n",
      "#make pickle files based on the csv for use in regression\n",
      "id_num = 99\n",
      "folder = 'maxes_medians'\n",
      "\n",
      "saveDir_data = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'no_short_windows', 'surr_' + str(id_num))\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows', 'surr_' + str(id_num))\n",
      "\n",
      "if not(os.path.exists(saveDir_data)):\n",
      "    os.mkdir(saveDir_data)\n",
      "else:\n",
      "    print(' %s\\n already exists!...' %(saveDir_data))\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats','single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "for row in df.groupby(['subj','task']):\n",
      "    subj, task = row[0]\n",
      "    elecs = row[1].elec\n",
      "    df_list = []\n",
      "    \n",
      "    for elec in elecs:\n",
      "\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_surr.csv']))\n",
      "        if os.path.exists(filename):\n",
      "            df_curr = pd.read_csv(filename)\n",
      "            df_list.append(df_curr)\n",
      "        else:\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "    \n",
      "    data_dict = dict(zip(elecs, df_list))\n",
      "    filename = os.path.join(saveDir_data, '%s_%s_surr.p' %(subj, task))\n",
      "    pickle.dump(data_dict, open(filename, 'wb'))\n",
      "    \n",
      "#all skipped elecs have windows <200 ms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "skipping CP9 DecisionAud 25\n",
        "skipping CP9 DecisionAud 24\n",
        "skipping CP9 DecisionAud 91\n",
        "skipping GP15 FaceGen 24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP15 SelfAud 12\n",
        "skipping GP15 SelfVis 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP27 SelfAud 39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 EmoRep 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceEmo 60\n",
        "skipping GP35 FaceGen 42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceGen 59\n",
        "skipping GP35 FaceGen 60\n",
        "skipping GP44 DecisionAud 233\n",
        "skipping GP44 DecisionAud 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 101\n",
        "skipping GP44 DecisionAud 143\n",
        "skipping GP44 DecisionAud 165\n",
        "skipping GP44 DecisionAud 65\n",
        "skipping GP44 DecisionAud 73"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 89\n",
        "skipping GP44 DecisionAud 129\n",
        "skipping GP44 DecisionAud 142\n",
        "skipping JH10 SelfAud 11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 EmoRep 39\n",
        "skipping JH17 EmoRep 73\n",
        "skipping JH17 EmoRep 26\n",
        "skipping JH17 SelfAud 99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 SelfVis 68\n",
        "skipping JH17 SelfVis 79\n",
        "skipping JH17 SelfVis 72\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 18\n",
        "skipping JH2 SelfVis 7\n",
        "skipping ST1 EmoGen 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST1 SelfAud 15\n",
        "skipping ST4 EmoRep 59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST4 EmoRep 60\n",
        "skipping ST4 SelfAud 22\n",
        "skipping ST4 SelfAud 58\n",
        "skipping ST4 SelfAud 59\n",
        "skipping ST4 SelfAud 60\n",
        "skipping ST4 SelfAud 19\n",
        "skipping ST6 SelfVis 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 402
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## with CIRCSHIFT\n",
      "folder = 'stds_only'\n",
      "\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'circshift')\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows', 'circshift')\n",
      "os.mkdir(saveDir_csv)\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "for row in df.itertuples():\n",
      "    i, subj, task, cluster, pattern, elec, start_idx, end_idx, start_idx_resp, end_idx_resp, _, _ = row\n",
      "    #load data\n",
      "    filename = os.path.join(SJdir, 'Subjs', subj, task, 'subj_globals.mat')\n",
      "    data_dict = loadmat.loadmat(filename)\n",
      "    srate = [data_dict.get(k) for k in ['srate']][0]\n",
      "    srate = float(srate)\n",
      "    min_window = round(200/1000*srate)\n",
      "    bl_st = -500/1000*srate #for all subj/tasks (since cue already accounted for in start/end_idx)\n",
      "\n",
      "    #skip specific elecs\n",
      "    if len(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec)>0:\n",
      "        if any(bad_df[bad_df.subj_task == '_'.join([subj, task])].elec == elec):\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "\n",
      "    if ((pattern == 'S') | (pattern == 'SR') | (pattern == 'sustained') | (pattern == 'S+sustained')) & ((end_idx - start_idx) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_circshift.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_circshift.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n",
      "\n",
      "    if (pattern == 'R') & ((end_idx_resp - start_idx_resp) > min_window): #keep elec\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_circshift.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_circshift.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n",
      "\n",
      "    if (pattern == 'D'):\n",
      "        trial_lengths = []\n",
      "        \n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_circshift.csv']))\n",
      "        df_trialvalues = pd.read_csv(filename)\n",
      "        start_idx = start_idx + abs(bl_st)\n",
      "        RTs = df_trialvalues.RTs\n",
      "\n",
      "        for i, r in enumerate(RTs):\n",
      "            trial_lengths.append(int(r+end_idx_resp-start_idx)) #length of each short trial so can use for long trial indexing\n",
      "        trials_to_drop = np.array(trial_lengths) < min_window\n",
      "        if any(trials_to_drop):\n",
      "            df_trialvalues = df_trialvalues[~trials_to_drop] #drop trials\n",
      "\n",
      "        filename = os.path.join(saveDir_csv, '_'.join([subj, task, 'e'+str(elec)+'_circshift.csv']))\n",
      "        df_trialvalues.to_csv(filename, index = False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "skipping GP15 SelfVis 1\n",
        "skipping GP35 FaceEmo 60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 233"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 296
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#with CIRCSHIFT\n",
      "#make pickle files based on the csv for use in regression\n",
      "folder = 'stds_only'\n",
      "\n",
      "saveDir_data = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'no_short_windows', 'circshift')\n",
      "readDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'elec_csvs', 'no_short_windows', 'circshift')\n",
      "\n",
      "if not(os.path.exists(saveDir_data)):\n",
      "    os.mkdir(saveDir_data)\n",
      "else:\n",
      "    print(' %s\\n already exists!...' %(saveDir_data))\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats','single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "for row in df.groupby(['subj','task']):\n",
      "    subj, task = row[0]\n",
      "    elecs = row[1].elec\n",
      "    df_list = []\n",
      "    \n",
      "    for elec in elecs:\n",
      "\n",
      "        filename = os.path.join(readDir_csv, '_'.join([subj, task, 'e'+str(elec), 'circshift.csv']))\n",
      "        if os.path.exists(filename):\n",
      "            df_curr = pd.read_csv(filename)\n",
      "            df_list.append(df_curr)\n",
      "        else:\n",
      "            print 'skipping %s %s %i' %(subj, task, elec)\n",
      "            continue\n",
      "    \n",
      "    data_dict = dict(zip(elecs, df_list))\n",
      "    filename = os.path.join(saveDir_data, '%s_%s_circshift.p' %(subj, task))\n",
      "    pickle.dump(data_dict, open(filename, 'wb'))\n",
      "    \n",
      "#all skipped elecs have windows <200 ms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "skipping CP9 DecisionAud 25\n",
        "skipping CP9 DecisionAud 24\n",
        "skipping CP9 DecisionAud 91\n",
        "skipping GP15 FaceGen 24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP15 SelfAud 12\n",
        "skipping GP15 SelfVis 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP27 SelfAud 39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 EmoRep 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceEmo 60\n",
        "skipping GP35 FaceGen 42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP35 FaceGen 59\n",
        "skipping GP35 FaceGen 60\n",
        "skipping GP44 DecisionAud 233\n",
        "skipping GP44 DecisionAud 19\n",
        "skipping GP44 DecisionAud 101"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping GP44 DecisionAud 143\n",
        "skipping GP44 DecisionAud 165\n",
        "skipping GP44 DecisionAud 65\n",
        "skipping GP44 DecisionAud 73\n",
        "skipping GP44 DecisionAud 89\n",
        "skipping GP44 DecisionAud 129\n",
        "skipping GP44 DecisionAud 142\n",
        "skipping JH10 SelfAud 11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 EmoRep 39\n",
        "skipping JH17 EmoRep 73\n",
        "skipping JH17 EmoRep 26\n",
        "skipping JH17 SelfAud 99"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH17 SelfVis 68\n",
        "skipping JH17 SelfVis 79\n",
        "skipping JH17 SelfVis 72\n",
        "skipping JH2 FaceEmo 113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping JH2 SelfAud 18\n",
        "skipping JH2 SelfVis 7\n",
        "skipping ST1 EmoGen 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST1 SelfAud 15\n",
        "skipping ST4 EmoRep 59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "skipping ST4 EmoRep 60\n",
        "skipping ST4 SelfAud 22\n",
        "skipping ST4 SelfAud 58\n",
        "skipping ST4 SelfAud 59\n",
        "skipping ST4 SelfAud 60\n",
        "skipping ST4 SelfAud 19\n",
        "skipping ST6 SelfVis 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 297
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Format data for Regression - no outlier rejection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_csvs', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df_all = pd.read_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##REAL DATA\n",
      "folder = 'maxes_medians'\n",
      "saveDir_data= os.path.join(SJdir,'PCA','Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'no_rejection')\n",
      "\n",
      "features_to_consider = ['maxes_rel','medians']\n",
      "predictor = 'RTs'\n",
      "features_plus_predictor = features_to_consider + [predictor]\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "    \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    for f in features_plus_predictor:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', '_'.join([subj, task, f]))\n",
      "        df = pd.read_csv(filename + '.csv')\n",
      "        df.columns = [int(x) for x in df.columns]\n",
      "        df_dict[f] = df\n",
      "\n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_plus_predictor]) #features x trials\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_plus_predictor)\n",
      "            \n",
      "    #filename = os.path.join(saveDir_data, '_'.join([subj, task]))\n",
      "    #pickle.dump(data_dict, open(filename + '.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##SURROGATE DATA \n",
      "#(if for INDICES- drop outliers based on non-idices surrogate data\n",
      "id_num = 60\n",
      "datafolder = 'maxes_medians'\n",
      "savefolder = 'maxes_medians'\n",
      "features_to_consider = ['maxes_rel','medians']\n",
      "predictor = 'RTs'\n",
      "features_plus_predictor = features_to_consider + [predictor]\n",
      "\n",
      "saveDir_csv = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'no_rejection', 'elec_csvs', 'surr_' + str(id_num))\n",
      "saveDir_data= os.path.join(SJdir,'PCA','Stats', 'outliers','for_Regression', 'unsmoothed', savefolder, 'no_rejection', 'surr_' + str(id_num))\n",
      "\n",
      "#os.mkdir(saveDir_csv)\n",
      "#os.mkdir(saveDir_data)\n",
      "#print('making\\n%s\\nand\\n%s' %(saveDir_csv, saveDir_data))\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "\n",
      "    subj, task = s_t[0] \n",
      "    \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "    \n",
      "    for f in features_plus_predictor:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files', 'surr_' + str(id_num), '_'.join([subj, task, f]) + '_surr')\n",
      "        df = pd.read_csv(filename + '.csv')\n",
      "        df.columns = [int(x) for x in df.columns]\n",
      "        df_dict[f] = df\n",
      "\n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_plus_predictor]) #features x trials\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_plus_predictor)\n",
      "        \n",
      "    filename = os.path.join(saveDir_data, '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '_surr.p', 'wb'))   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##CIRCSHIFT DATA\n",
      "folder = 'maxes_medians'\n",
      "saveDir_data= os.path.join(SJdir,'PCA','Stats', 'outliers','for_Regression', 'unsmoothed', folder, 'no_rejection', 'circshift')\n",
      "\n",
      "features_to_consider = ['maxes_rel','medians']\n",
      "predictor = 'RTs'\n",
      "features_plus_predictor = features_to_consider + [predictor]\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "    \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    for f in features_plus_predictor:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'unsmoothed', 'csv_files','circshift', '_'.join([subj, task, f, 'circshift']))\n",
      "        df = pd.read_csv(filename + '.csv')\n",
      "        df.columns = [int(x) for x in df.columns]\n",
      "        df_dict[f] = df\n",
      "\n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_plus_predictor]) #features x trials\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_plus_predictor)\n",
      "            \n",
      "    filename = os.path.join(saveDir_data, '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '_circ.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Correlation between maxes and latencies outlier rejection\n",
      "* all features (relative maxes and real latencies) have the same number of trials (across electrodes)\n",
      "* each electrode can have a different number of trials"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_to_consider = ['maxes_rel', 'lats']\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "        \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "\n",
      "    subj, task = s_t[0]\n",
      "    \n",
      "    #find outlier trials\n",
      "    for f in features_to_consider:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'static', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "        \n",
      "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "        \n",
      "        masked_values = np.where(~trials_to_drop, df.values, np.nan) #good trials\n",
      "              \n",
      "        tmp = pd.DataFrame(masked_values, **df._construct_axes_dict()) #new dataframe with dropped trials for 1 feature\n",
      "        tmp.columns = [int(x) for x in tmp.columns]\n",
      "        df_dict[f] = tmp\n",
      "        \n",
      "    #drop nans for all features for an electrode    \n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_to_consider]) #features x trials\n",
      "\n",
      "        trials_to_drop = np.any(np.isnan(data_array), axis = 0)\n",
      "        \n",
      "        if np.any(trials_to_drop):\n",
      "            tmp = [features_plus_predictor[i] for i in np.where(np.isnan(data_array))[0]]\n",
      "            feature_list.append(tmp)\n",
      "        else:\n",
      "            feature_list.append('')\n",
      "            \n",
      "        data_array = data_array[:,~trials_to_drop]\n",
      "        \n",
      "        data_dict[elec] = pd.DataFrame(data_array.T, columns = features_to_consider)\n",
      "        \n",
      "        filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Correlation','static','maxes_vs_lats','elec_csvs', '_'.join([subj, task, 'e'+str(elec)+'.csv']))\n",
      "        data_dict[elec].to_csv(filename, index = False)\n",
      "        \n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        num_total.append(trials_to_drop.shape[0])\n",
      "        num_to_drop.append(sum(trials_to_drop))\n",
      "        all_elecs.append(elec)\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Correlation','static', 'maxes_vs_lats', '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Correlation between RTs and latencies outlier rejection\n",
      "* all features (RTs and real latencies) have the same number of trials (across electrodes)\n",
      "* each electrode can have a different number of trials\n",
      "* shifts RTs by baseline and latencies by HG onset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_withdesignation_EDITED.csv')\n",
      "df_all = pd.read_csv(filename)\n",
      "\n",
      "features_to_consider = ['RTs', 'lats']\n",
      "\n",
      "subjs, tasks, num_total, num_to_drop, feature_list, all_elecs = [[] for i in range(6)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "        \n",
      "    df_dict, data_dict = [dict() for d in range(2)]\n",
      "    subj, task = s_t[0]\n",
      "        \n",
      "    #find outlier trials\n",
      "    for f in features_to_consider:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows','static', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "\n",
      "        trials_to_drop = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "        \n",
      "        masked_values = np.where(~trials_to_drop, df.values, np.nan) #good trials\n",
      "              \n",
      "        tmp = pd.DataFrame(masked_values, **df._construct_axes_dict()) #new dataframe with dropped trials for 1 feature\n",
      "        tmp.columns = [int(x) for x in tmp.columns]\n",
      "        df_dict[f] = tmp\n",
      "        \n",
      "    #drop nans for all features for an electrode    \n",
      "    for elec in df_dict[f].columns:\n",
      "        \n",
      "        #pickle file with latencies per subj/task\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'static', 'data', ''.join([subj, '_', task, '.p']))\n",
      "        tmp_dict = pickle.load( open(filename, \"rb\" ) )\n",
      "    \n",
      "        #calculate onset and baseline in order to shift latencies and RTs by baseline and HG onset\n",
      "        onset = df_all[(df_all.subj == subj) & (df_all.task == task) & (df_all.elec == elec)].start_idx.values[0]\n",
      "        bl_st = tmp_dict['bl_st']\n",
      "\n",
      "        \n",
      "        data_array = np.array([df_dict[x][elec] for x in features_to_consider]) #features x trials\n",
      "\n",
      "        trials_to_drop = np.any(np.isnan(data_array), axis = 0)\n",
      "        \n",
      "        if np.any(trials_to_drop):\n",
      "            tmp = [features_to_consider[i] for i in np.where(np.isnan(data_array))[0]]\n",
      "            feature_list.append(tmp)\n",
      "        else:\n",
      "            feature_list.append('')\n",
      "            \n",
      "        data_array = data_array[:,~trials_to_drop]\n",
      "        \n",
      "        df = pd.DataFrame(data_array.T, columns = features_to_consider)\n",
      "        df['RTs'] = df['RTs'] - abs(bl_st) #remove bl from data (start from stim onset)\n",
      "        df['lats'] = df['lats']+onset #add HG onset back on so calculated from stim onset\n",
      "        \n",
      "        data_dict[elec] = df\n",
      "        \n",
      "        filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Correlation','static','RTs_vs_lats','elec_csvs', '_'.join([subj, task, 'e'+str(elec)+'.csv']))\n",
      "        data_dict[elec].to_csv(filename, index = False)\n",
      "        \n",
      "        subjs.append(subj)\n",
      "        tasks.append(task)\n",
      "        num_total.append(trials_to_drop.shape[0])\n",
      "        num_to_drop.append(sum(trials_to_drop))\n",
      "        all_elecs.append(elec)\n",
      "    \n",
      "    filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers','for_Correlation', 'static', 'RTs_vs_lats', '_'.join([subj, task]))\n",
      "    pickle.dump(data_dict, open(filename + '.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##strict older version of outlier rejection\n",
      "* all electrodes and all features have the same number of trials for a give subject/task\n",
      "* outliers removed from all features, not just those that had the outlier themselvs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subjs, tasks, num_total, num_to_drop = [[] for i in range(4)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "        \n",
      "    subj, task = s_t[0]\n",
      "\n",
      "    #find outlier trials\n",
      "    for j, f in enumerate(features_to_consider):\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "        \n",
      "        tmp = df.apply(lambda x: ((x > (x.mean() + x.std(ddof = 1)*std_thresh)) | (x < (x.mean() - x.std(ddof = 1)* std_thresh))))\n",
      "        \n",
      "        if j == 0:\n",
      "            trials_to_drop = tmp\n",
      "        else:\n",
      "            trials_to_drop = trials_to_drop | tmp #combine outlier trials for different features that considering\n",
      "    \n",
      "    outliers = np.where(trials_to_drop)[0]\n",
      "    \n",
      "    subjs.append(subj)\n",
      "    tasks.append(task)\n",
      "    num_total.append(df.shape[0])\n",
      "    num_to_drop.append(len(outliers))\n",
      "    \n",
      "    #drop outliers\n",
      "    for f in features:\n",
      "        filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'csv_files', '_'.join([subj, task, f]) + '.csv')\n",
      "        df = pd.read_csv(filename)\n",
      "        \n",
      "        df.iloc[outliers] = np.nan\n",
      "        df = df.dropna()\n",
      "\n",
      "        filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers', '_'.join([subj, task, f+'.csv']))\n",
      "        df.to_csv(filename, index = False)\n",
      "        \n",
      "df_tally = pd.DataFrame({'subj':subjs, 'task': tasks, 'total' : num_total, 'outliers' : num_to_drop})\n",
      "df_tally['percent'] = df_tally['outliers']/df_tally['total'] * 100\n",
      "df_tally = df_tally[['subj','task','outliers','total','percent']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 597
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir, 'PCA', 'Stats', 'outliers', 'tally_all.csv')\n",
      "df_tally.to_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 594
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## calculate number of elecs in each subj/task"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subjs, tasks, num_elecs = [[] for i in range(3)]\n",
      "\n",
      "for s_t in df_all.groupby(['subj','task']):\n",
      "        \n",
      "    subj, task = s_t[0]\n",
      "\n",
      "    #calculate number of elecs\n",
      "    filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'csv_files', '_'.join([subj, task, 'RTs']) + '.csv')\n",
      "    df = pd.read_csv(filename)\n",
      "    \n",
      "    subjs.append(subj)\n",
      "    tasks.append(task)\n",
      "    num_elecs.append(len(df.columns))\n",
      "        \n",
      "df_elecs = pd.DataFrame({'subj':subjs, 'task': tasks, 'num_elecs' : num_elecs})\n",
      "\n",
      "filename = os.path.join(SJdir, 'PCA', 'num_elecs_all.csv')\n",
      "df_elecs.to_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 514
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.where(tmp)\n",
      "tmp.iloc[np.where(tmp)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(array([ 69,  69, 105, 111]), array([ 9, 19,  3, 18]))\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>21</th>\n",
        "      <th>42</th>\n",
        "      <th>12</th>\n",
        "      <th>38</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>69 </th>\n",
        "      <td>  True</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>69 </th>\n",
        "      <td>  True</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>105</th>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>111</th>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td>  True</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 426,
       "text": [
        "        21     42     12     38\n",
        "69    True   True  False  False\n",
        "69    True   True  False  False\n",
        "105  False  False   True  False\n",
        "111  False  False  False   True"
       ]
      }
     ],
     "prompt_number": 426
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## elecs with multitask sensitivity\n",
      "** includes elecs that only active in 1 task **\n",
      "* elecs common to all auditory tasks (EmoGen, EmoRep, SelfAud)\n",
      "* elecs common to visual and auditory language tasks (SelfVis, SelfAud, EmoGen)\n",
      "* elecs common to linguistic and non-linguistic tasks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta'\n",
      "df_pattern = pd.read_csv(os.path.join(SJdir, 'PCA','Stats', 'single_electrode_windows_withdesignation_EDITED_dropped_withROI.csv'))\n",
      "df = df_pattern[['subj','task','elec' ,'pattern','ROI']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#contrast = 'auditory_tasks'\n",
      "#df_aud = df.query(\"task == 'EmoGen' or task == 'SelfAud' or task == 'EmoRep'\")\n",
      "#df_all = pd.DataFrame({'subj':[], 'elec':[], 'EmoGen':[], 'EmoRep':[], 'SelfAud':[]})\n",
      "\n",
      "#contrast = 'visual_auditory_tasks'\n",
      "#df_aud = df.query(\"task == 'EmoGen' or task == 'SelfAud' or task == 'SelfVis'\")\n",
      "#df_all = pd.DataFrame({'subj':[], 'elec':[], 'EmoGen':[], 'SelfAud':[], 'SelfVis':[]})\n",
      "\n",
      "contrast = 'linguistic_nonlinguistic'\n",
      "df_aud = df.query(\"task == 'EmoGen' or task == 'SelfAud' or task == 'EmoRep' or task == 'SelfVis' or task == 'FaceEmo' or task == 'FaceGen'\")\n",
      "df_all = pd.DataFrame({'subj':[], 'elec':[], 'EmoGen':[], 'EmoRep':[], 'SelfAud':[], 'SelfVis':[], 'FaceEmo':[], 'FaceGen':[]})\n",
      "\n",
      "\n",
      "for row in df_aud.drop_duplicates('subj').itertuples():\n",
      "    _, s, t, e, pattern, ROI = row\n",
      "\n",
      "    df_subj = df_aud.loc[(df.subj == s)]\n",
      "    elecs = df_subj.drop_duplicates('elec').elec\n",
      "    \n",
      "    #to only include elecs that active in >1 task (not necessary)\n",
      "    #counts = df_subj.elec.value_counts() #number ot times elec appears\n",
      "    #elecs = counts[counts>1].index #repeated elecs\n",
      "    #df_multi = df_subj[df_subj['elec'].isin(elecs)].sort('elec')\n",
      "    #df_multi['dummy'] = 1\n",
      "    \n",
      "    #for each elec, indicate the pattern that it has in each task\n",
      "    df_multi = df_subj\n",
      "    df_multi = df_multi.pivot(index = 'elec', columns = 'task', values = 'pattern')\n",
      "\n",
      "    df_multi['subj'] = s\n",
      "    df_multi = df_multi.reset_index()\n",
      "\n",
      "    colnames = list(df_multi.columns)\n",
      "    colnames.pop(colnames.index('subj'))\n",
      "    colnames.insert(0, 'subj')\n",
      "    df_multi = df_multi[colnames]\n",
      "    df_all = df_all.merge(df_multi, how = 'outer')\n",
      "\n",
      "    \n",
      "filename = os.path.join(SJdir, 'PCA','Stats', 'multi_elecs', contrast+'.csv')\n",
      "df_all.to_csv(filename, index = False)\n",
      "\n",
      "filename = os.path.join(SJdir, 'PCA','Stats', 'multi_elecs', contrast+'_withROI.csv')\n",
      "df_all_ROI = df_all.merge(df_pattern[['subj','elec','ROI']], how = 'inner').drop_duplicates(['subj','elec'])\n",
      "df_all_ROI.to_csv(filename, index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 225
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## compare elecs with outliers to elecs with trials dropped (use PCA/Stats/outlier_elecs.txt and _dropped.csv) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#format text file into dataframe (for elecs - drop 'e' and convert to int)\n",
      "df_out = pd.read_table('/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/outlier_elecs.txt', header = None, sep = ' ')\n",
      "df_out.columns = ['subj','task','elec']\n",
      "df_out = df_out.dropna()\n",
      "df_out['elec'] = df_out['elec'].map(lambda row: int(row[1:]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_drop = pd.read_csv('/home/knight/matar/MATLAB/DATA/Avgusta/PCA/Stats/single_electrode_windows_withdesignation_EDITED_dropped.csv')\n",
      "df_drop = df_drop.sort(columns='dropped',ascending = False)\n",
      "df_drop = df_drop[['subj','task','elec','dropped']]\n",
      "df_drop = df_drop[df_drop['dropped']>0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_drop"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>subj</th>\n",
        "      <th>task</th>\n",
        "      <th>elec</th>\n",
        "      <th>dropped</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>120</th>\n",
        "      <td> GP44</td>\n",
        "      <td> DecisionAud</td>\n",
        "      <td> 233</td>\n",
        "      <td> 50</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>51 </th>\n",
        "      <td> GP15</td>\n",
        "      <td>     SelfVis</td>\n",
        "      <td>   1</td>\n",
        "      <td> 20</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>173</th>\n",
        "      <td>  JH2</td>\n",
        "      <td>     FaceEmo</td>\n",
        "      <td> 113</td>\n",
        "      <td> 16</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>97 </th>\n",
        "      <td> GP35</td>\n",
        "      <td>     FaceEmo</td>\n",
        "      <td>  60</td>\n",
        "      <td>  8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>204</th>\n",
        "      <td>  JH5</td>\n",
        "      <td>      EmoRep</td>\n",
        "      <td>   2</td>\n",
        "      <td>  6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>170</th>\n",
        "      <td>  JH2</td>\n",
        "      <td>      EmoGen</td>\n",
        "      <td>  18</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>146</th>\n",
        "      <td> JH17</td>\n",
        "      <td>     SelfVis</td>\n",
        "      <td>  59</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>124</th>\n",
        "      <td> JH10</td>\n",
        "      <td>     SelfAud</td>\n",
        "      <td>  59</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6  </th>\n",
        "      <td> GP15</td>\n",
        "      <td>      EmoGen</td>\n",
        "      <td>  27</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>160</th>\n",
        "      <td>  JH2</td>\n",
        "      <td>      EmoGen</td>\n",
        "      <td>   5</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>200</th>\n",
        "      <td>  JH2</td>\n",
        "      <td>     SelfVis</td>\n",
        "      <td>  20</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>199</th>\n",
        "      <td>  JH2</td>\n",
        "      <td>     SelfVis</td>\n",
        "      <td>   3</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>169</th>\n",
        "      <td>  JH2</td>\n",
        "      <td>      EmoGen</td>\n",
        "      <td>  11</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>139</th>\n",
        "      <td> JH17</td>\n",
        "      <td>     SelfVis</td>\n",
        "      <td>  17</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "     subj         task  elec  dropped\n",
        "120  GP44  DecisionAud   233       50\n",
        "51   GP15      SelfVis     1       20\n",
        "173   JH2      FaceEmo   113       16\n",
        "97   GP35      FaceEmo    60        8\n",
        "204   JH5       EmoRep     2        6\n",
        "170   JH2       EmoGen    18        2\n",
        "146  JH17      SelfVis    59        2\n",
        "124  JH10      SelfAud    59        2\n",
        "6    GP15       EmoGen    27        2\n",
        "160   JH2       EmoGen     5        2\n",
        "200   JH2      SelfVis    20        2\n",
        "199   JH2      SelfVis     3        2\n",
        "169   JH2       EmoGen    11        2\n",
        "139  JH17      SelfVis    17        2"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combo = [''.join([x, '_', y, '_', str(z)]) for x, y, z in zip(df_drop['subj'].values, df_drop['task'].values, df_drop['elec'].values)]\n",
      "df_drop['combo'] = combo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combo = [''.join([x, '_', y, '_', str(z)]) for x, y, z in zip(df_out['subj'].values, df_out['task'].values, df_out['elec'].values)]\n",
      "df_out['combo'] = combo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_drop.combo.isin(df_out.combo)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "120    False\n",
        "51     False\n",
        "173    False\n",
        "97     False\n",
        "204    False\n",
        "170    False\n",
        "146    False\n",
        "124    False\n",
        "6      False\n",
        "160     True\n",
        "200    False\n",
        "199     True\n",
        "169    False\n",
        "139    False\n",
        "Name: combo, dtype: bool"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_out.query(\"subj == 'JH2' and task == 'EmoGen' and elec == 5\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>subj</th>\n",
        "      <th>task</th>\n",
        "      <th>elec</th>\n",
        "      <th>combo</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>100</th>\n",
        "      <td> JH2</td>\n",
        "      <td> EmoGen</td>\n",
        "      <td> 5</td>\n",
        "      <td> JH2_EmoGen_5</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "    subj    task  elec         combo\n",
        "100  JH2  EmoGen     5  JH2_EmoGen_5"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_out.query(\"subj == 'GP15' and task == 'SelfVis' and elec == 1\")\n",
      "df_out.query(\"subj == 'GP15' and task == 'EmoGen' and elec == 27\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>subj</th>\n",
        "      <th>task</th>\n",
        "      <th>elec</th>\n",
        "      <th>combo</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "Empty DataFrame\n",
        "Columns: [subj, task, elec, combo]\n",
        "Index: []"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# OLD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f,ax = plt.subplots()\n",
      "ax.autoscale(enable = True, tight = True)\n",
      "cax = ax.pcolormesh(np.arange(bl_st, data_dur.shape[1]+bl_st), np.arange(0, data_dur.shape[0]), data_dur, zorder = 0)\n",
      "cbar = f.colorbar(cax, ticks = [-150, 0 , 150], orientation = 'vertical')\n",
      "cax.set_clim(vmin=-150,vmax=150)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAEACAYAAABxgIfcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXuwJPdV3z9XM5rxTPaSjeVCz0W7+FHYLgcwYF4GrUEh\nwlA2FBUbp0gpxpVKhfAwKHilTRYupcS2RMnmVZBQYJdNgbASUopd4WGZWLJdxA/ANg5CseWSjCRH\nq0T2mrs14xnNaPLHr0/3+Z3+dc/Mfc30vedTNTUz/fz13N3Tp7+/8wDHcRzHcRzHcRzHcRzHcRzH\ncRzHcRzHcRzHcRzHcRzHcZw95W3AeeBTatkW8Cjw8ez1vWrdLcBngAeA7zmYITqO4xxdvgP4emIj\n/fPAzyS2fQHwCeBS4CTwIHDJbgew6wM4juMcYj4IfDGxfCOx7JXAncBTwMMEI/2S3Q7AjbTjOM7y\n/ATwSeC3gePZsqsIMojwKHD1bk/kRtpxHGc5fgM4BXwd8H+AO2q2ne32ZO3dHmBZTlz3VbNH7vvb\ngz6t4zjN5D7g9G4O8AyYfXnxzbeBr5izzRPq828B78k+PwacUOuuyZbtipSust/Mzs7O0WIKwJQW\nU1oA2adptE4IyydMaautWtE+envhvq0P8NKtl+XrOoyjY43pZN9adBnTYRyNS48N4I0b5XOshvcD\nL1v1IPYBv65msd/XtQW7t1Ozf7/ghv8uvNnznSQY4hdl368keNAAPw18E/BPCROHv0fQoa8G3gc8\nh1160wfuSQNsss0oM4kjOtlAghFs5e8TAMbZelnWZwCQG1bZV4y0HFd4OlN07E1hSi9f3mOYn08M\nd9kYr4txdhxnWS7d+a53AtcBzwIeIUR2nCZIHTPgIeBfZtveD9yVvU+AH6OJcgfAgH5uKvsMI691\nSK/kI09oMaVNi17kRwsX6eQGO1xUvB7Aeugtpty60SEY395BXLbjOCtiF4buNYllb6vZ/o3Za8+Y\nN3GYCuQWbgKeBp6plq1dIPe1p69d9RD2iZOrHsA+cXLVA9gnTq56APvEyVUPYCF6C77WkXk3mLcD\nvwq80yw/Afwj4HNq2QuAV2fvosc8j2DIIzqMGNONNN9iXRBBRP4Q+WIa+dvTXEuGQrIQ9PLnnr4K\nMp1ZEGnk7Gwa+ex6X1kurI8WLZxa9QD2Cb+uZtGM69qF3LFy5hnpD5K+Vb4FeAPw39SyqkDuD9ud\nxUBDerIwrA9CR5dRpBfL51723jb7DugzpqO07WJdkDdStEhrzutmmB3H2Qkr0XX3iJ2M/ZWEIO2/\nMsuvIjbIexLI7TiOs1sOsydt6QNnCVKHUBceUzmz2VEShA2n08umZgJwTDfbfwTARHnP+thaqhBp\n5KZZywTw2XPF3r1eN6nwqttMa8P6UhOdNqxP5Bc7USp0GUWhgQP6+e/VZ8Am2/n6AT2Gar0+p1zD\nHRvxb+o4h52j5Ek/myB/fDL7fg3wF8A3s0Qg9we27mPGBk9zCSdOn+Kq08+JjJ0QDFIvG+g0Muwi\nmYyUcZY45yJ8L9a925RjsydmHcCITsKQ1Rm2KhlF9llWTpkmzqfPMTLHOE4ae3w3zs668xBBLd1b\njpIn/SngcvX9IeAbgC8A7yYEcr+FIHM8F/ho6iAv23qp8Rwdx3EgTETqycj79uSoh9lISyD3ZYRA\n7p8jRHwIWs5YOJC7yyh/pBeZYkKLSRYHDYXkoOOotVfdY8gm27lUIBKArO8wpss4kkL0bSH2nIuJ\nTAg+67ls5DbeWkd/6HfBbq+XW1lDlqekH7uvzb6027eIb3sil2xtrGtgkeMcHE3+X7CStPCbZrfS\nzQxTKgVcf7e6bmGUe0iKeCcL1Ovl2YjtKJtRSB1nnO2tNd4eg9yIT/Jok7RmrQ2vjNvq11XJNfpa\n45tEtdGu+r3sMatIyTxVmrm91tQNJLV96vrWL4TRaQZbsAdp4e9fcMMswX0VdrGSJuvpjuM4C3GY\n5Y59ocs46ZEJIgHYgkphXajfcZwLjOnkckZIKS/SzTuMK7y31CSenYjbnHMFVROBQsqjXaYWSNVE\n5Lz9lvFW9RjrrmdZD7hl3h1n9TTZG13J2LUsAeQasTwijzJFWSSIjjKiU9oMaNPNlvUZRJqt1pxv\nnw1y+WFqLjUlMch+IyVvyPnlxjKiy5BeLsPYDEktjxTjKaZH9ThsUSerLdtx2mOlZBj9bve/ba0e\n4hzn4HBP2nEcZ41psqFbydgvshl5e63cFy57myHFu/B4xSvUqd+yfEQHSR3XtaFTySXiAcv5wja9\naBvZd6DmhuW4PTVheJHNGi91J3LBTqUHlxocJ4V70kvSy8LqoCw7dDNpQbByhZYzdLGlCa0oKkLv\nHwowDegzzGqBTKPjTmmV6lSnpJiU3CFjOjcrR0hI6F8njyEJkono6BJh0c0zFuOocRnbPKrOK+fW\n61Lbz4vmkJua/u2k8JXIQTqkUUe32DkFOUe6iFaRJZmKYklFoVRJQXJuW4irw9jDEo8gTf6LN/kp\nwHEcZyHck14S7YGJpyWe5ZgOPQZJr7OQIYKnKB6SpJPbRBEoPLIh/bymBVTHX5c93BZjCjkFQmxx\nN4oemRft0SEdsTEvbXwvSJ13XoRIKhKlquJuq+IcddTtUyfZ7DayxRs8HFWa7I02eeyO4zgLcemi\nlm4Na1SsxEiLjjzI0sB7DLmMJ3NvdkCfbRWCpwv6h/2LtGfRHG342lh52+lJvTovtmqdjS12HKcJ\ntN1IL4cYXpEXtjnGNseS6dNxI1qdjl0U/7cTbC2mHGObNlMmtNia1U/AVUkltgRpSkrRE6B60ktH\nh+jIEllfNxaNnRiTMYj0IpOOdnt9rNQEm449t3HdVedPHasqhV2fQ9doScWN+0Ses99c2mCfyuUO\nx3EOPQt70mvISoZe9lbTRXv05J5MJkKRkWhD3HTWX/DeupXnrcrqs5OQ48QEV1VIms16DMcjGqek\ntEvW5UU2+SLHGdLP97VF/iX+W47fy4pAQeEFWw/ZhhDqqoPykvG0s2PZTEsbKqdDE4P3WzcRVzU5\n2GCXxmksl3bnb7OurMRID2pm2MUYtYgr0o3o8iSbeUlSSePuMs71bdvNxMbVWrnAGrJw/qKKnU0l\nl+1CVMdO+gtLlEdVkX4WPO5eGb+qqJOdHs9x1hT3pB3HcdaYBlu6lVXBg3R2n3jAsv6C8jrbTNlk\nO3/sn2QesU4BFy95RBebHl5MWUkc9CjytMMYppFvqfcJXn6b1+cNASZRW6/CS+9G62wMdiqbMfby\n09KJ/h1kHRDJIZ08f6+cEm9JTdRa9LXrJ466/bx2tLN2uJFeDq0Nh+iEYW6MdBSF1Y0FHXUh2vSI\nTr5nuAmM8+MMM/lAjqXTzvWxxVCmDLdGp3Fr3Vq06mNsq2OnrmccXRvYDiq6dGod8pvYnogprbjK\nqC5iUFM9F90QOw2iwf9cG3x/cRzHWZAGW7qVDP0yngREmoinXbvKi7S1mWUfeW8Rx/BOKFe6096q\neLxaIpF+i2O6UQ9Fu3+QEooxVMVvTwlV8fS6FNq7Bjg3Kzx/u1/qKSKFLSaUiteWY9gCRzKWLqMo\nckQ3Vegw5taNZVPAHWcN8OiO5XiSy/LPYkQ6jHPNVxuwDiPObdhOKVX1HRbtNjIlXY9inmRQxV6U\nF110+aJGsm5MdVEdPaqjT9xAOw3FPWnHcZw1psGWbiVDt+2sisSPMUW6cCuPfz4z0/JDXC/YotO7\ndURDLGGECUqZcNSNAsSz98d6xzlEHOKJw7cB3wc8AbwoW/aLwPcDY+CzwGuBL2XrbgF+lKAb/CTw\n3tRBj3OhJGtArPUWckcRyaEz58pZgsF46wiRFq3IZMtyXbRfoktsDY5zsyKsLhy3yL7T2XtT0n0J\nZZ9UEf8WU69X4TgHSYM96UvmrH87cINZ9l7ghcDXAp8mGGaAFwCvzt5vAH59geM7juPsP+0FX2vI\nvGF9EDhplt2jPn8E+KHs8yuBO4GngIeBB4GXAB+2B73IJj2GeZ0JieAYqYkpnaYt1TtEvmgbr3VI\nL/+eSgMX71zvX0Q2FOnhYZ/C89UtoYpxluO2ZYzejdtx1pQ1NcCLsNuh/yjBMANcRWyQHwWurtpR\nDKsU+QHYpOh3B4XEoDMU9XrZpgobjtZTsodIFZL4IgZd9ksVYJIwtDCmUSR7AJydxeVL9bn1mFJh\ndvq8en/9OxTbtbGSir4mv1k4juGIhuD9W4Iu/Xs128xSCz+09X4ANpjx7NNX8zWnL9/FMBzHOTw8\nRHgQ32OOoCf9z4GXA9+tlj0GnFDfr8mWlbhh6xsjb3CUSxUhHUX8ZJEedEKFLluaqnMB5dKjth4I\nVDcL0O96uV4n8dz6eHG/w3jvahbt47eTPoKO00ROZS/hvr057CGO7khxA/CzwHXAl9XydxO86rcQ\nZI7nAh9NHcCGzmmZQRBjqL/Le9CG27lRl33L3T/CPgPVgFav18e1hl53DtFGv9g/Lnp0blYvabSY\nelif46yKQ+xJ30kwxs8CHgF+nhDN0aGYQPyfwI8B9wN3Ze+TbFlS7nAcxzlQDrGRfk1i2dtqtn9j\n9qol1dFE6kbYZBOpqwGUklnCBUxz6WOQtQkA6DPM64DIZKOWSiB42BIzfcdGfcnOgmWfm6pkEMdx\nDowG//dbWcYhBAmgzwDJDgxdwjcjacEWOdJlR3sM8nVFjehxHg2hE196mdHWNwed1HJuFp8nlcQi\n50lp2ZKxKPvrwlG2BKuEGkq9aXt8XVTKazM7zh5wiD1px3Gc5vOMVQ9g56yo6H8RHaE7h+guJakY\nZUHiq6GYFLTb6Ca0Mqk4pJd7riGKZAyZpDKhF9XuKI7TyWUXIeUBj+hyQU1Q6qYAOjKjxZRNLqK7\nssj1y5h1XPTWrKgxItgyq/PiplM1S1Kx2qlkIvm76GsS7PXbCBy9ztPgnZXS4AfSlRhpmyHYoVxL\nWUdQWHkhLJNtJ7mxgiJ7MZYrbCeWdNRIqgO47vRtGdNlSKt0DE1Vso3sa2t/iCEdKoPfYZwbdj3+\nQdZh3J5L1svvGs7XYZr97nqc+rdK3aQ65tjFb9jKO5yn/jZDM7azs3J4o0s6zoHRYM2gwUN3HMdZ\nkAZbupUMvccQWx2u+FykX1dhq+JpySMll6TSyLWkoM+vm8h2M1nEdodJefoaK79Uedj9bOJTe+3T\nTJLZzPokFm1lO3vUGaWuGQBz1i17nv08vuMsQYP/2TX4/uI4jrMgDbZ0K9OkNSlNVzzgsk5d6K5j\nuiWPWBoFANFEZCr9206i2fV2mQ0F1FX1UujttzmWn1OH69nwPJnEFM9ZJuFkrGdn8TV5FqPjLIAb\n6eXoMQTisqBQru4m1fGqiux3GEXGusWETUYcV8ZxlJQqJipiYVKSK4pti24tEt8s26aMfyc3vOWK\neoIuwTrOeg3K+k62RE/2hXXxjUQfUybkZL0t51oV+20n8OQGocdp9xckISj1u8jvJlJU2LedL5f9\n5He/aeMrk7+94+wpR7QKnuM4TjNosKVb6dDHUVaenogLcsIwChkbsZmF2o2zbEHZTzxYHdYm64pj\na88x7kqeisEWb3CUBealPFFd47kq9K/KkxWCXLGTiby6Tuh6m6qqeqnlVrap2s52b3ecNceN9M6w\nPQOt3GGN6FQZkdgAF/JFVZPbVFp3VYSFvgHo8+gEDSsNtBnk0kps9As5ZYoU5G/wVLPjNJEG/5dr\n8P3FcRxnQRps6VYydJkQEy9XWljJ5JXOmLPp0ylvdZzHSpRJyRl63YC+mjab0EJnwdnH/WUjKTwV\n2nHWAjfSy1FERrSxiSzyrqMNdK0M6dySCp2TfevTjBv83OM4zs5o8H/7Bt9fHMdxFsSr4C1Hnces\nk1Y6aptBluAh63oMOLexyfw0Z8dxjjwNdkdXZqRTZTGr1gWjPERn5w3pcyZrzlWVIZj6HofRTaJz\n24L7sl1K17ahdaKlxwkb1eVWU+NMra8idc3LkK5FUo6oSZHKtNRRMPocVb+NHYdXwnP2lQb/82rw\n/cVxHGdBGmzpVjL0ELVR9tZs8omNox4mun7bvodjugskiCzDoskgU4pkkGUkmJ2Ocy9dg2Ulo5QX\nX9fLcae/jePsEW6kl6NKPpDHbelEMqBHnyGbbNPJSneO6TKgn5cftYkjLaZszYaRrKEb3I6zb7qe\nRF3/QjlHqt6FYG8mqdKo834PjZYdbNKMMDH7WOmhaqxV40nJFKnl8i43SGn2a6UiffPU2Z365gtx\nRxfZr5MtlZDLumJWVZ1jXEZxIhr8z6DB9xfHcZwFOcTRHW8Dvg94AnhRtuyZwLuAa4GHgVcBF7J1\ntwA/Sni2/UngvamDXsaTWUnRftK76jCK2i+NlKclySx2oqqoKke+XHzRovxnN/IQZd+UFygeonT9\nlqOJh6j7BV5kM+nxikesx6mpLjOqb/tVnnidh14nKSzrUtQdp0d1wk7V8hb1SUGdivWL/A4upTgV\nNPifxTwj/XbgV4F3qmU3A/cAtwNnsu83Ay8AXp29Xw28D3ge8LQ96AWO5591iF0YUFwiU5JexsZY\ntpjuMmmlbv1OjE+dTrvIeBzH2TcarBlcMmf9B4EvmmWvAN6RfX4H8APZ51cCdwJPETzsB4GX7Mko\nHcdxdkN7wdcaspNhXQ6czz6fz74DXAV8WG33KMGjLqG7jhxTE4JWikj1ENSlSW+dxX0SUxNeWmbQ\nx0tNVukJxKrx2IlFO7Gm46/lOGM6pYSdquNqyUaXOpXjQ51M4jhOkjU1wIuw26HPslfd+hIf3foT\nnuYSprQ4cfoUX3X6VGYox5HWO83qSVvGRrMUw607uYhxlPXWWHfUsaTlVttsJxTGP27nlYpgkKzI\nuka6sr8+bpFJOczPKe9iyEUn35rFHWH0tlXRDsV7OeJCb1sVkaHPJclGsoVueTa/dorj1PEQ4UF8\nj2nwP8mdGOnzwBXA48CVhElFgMeAE2q7a7JlJb5r69tK3qzjOA6cyl7CfXtz2CPmSb8buBG4LXu/\nWy3/PeAtBJnjucBHUwewUgKkvVVZrrEp4kVMdT+Ph+4wos9A9UDsGK+xFR07FOLXyRaLINENi66b\nF4WwVzernUY7LDMJa8/hJVmdNecQ9zi8E7gOeBbwCPBzwJuBu4DXUYTgAdyfLb8fmAA/RoXcYZMV\nrCxhO2jb7MOxSnDoMmaTi0Zv7iJdXLY2UgbEvXfHOVIcYk/6NRXLr69Y/sbs5TiOsz4cYiO9LzzJ\ns7AV6IBk9TsIssRF1fxUJqt0KVMrkYh3fmZWTheuSguHdIU3vTw1trrUban+ZveR65B3PfmnsZEg\nQC7xyLH7DKNejZI2L5OIEiUiv5P+nedV7LMTqXK99verO0fd72aPk4rK0ehIGX1MfTw79i6j0mSu\nSGATWtyxsbuKgk4DcCPtOI6zvswarHCuxEhfxv+LNGfRnXsMo4I9khreYspmFk8tGraEuvUY5B0O\nR1lCt+yj3wUJ9QNKXl+bKf0sBK4ci2y9rUVTt+0kW+oYi0za1VXdm5dKXXct8yYX51W1E+b9Xqnj\npY6zyG9StU/qeHqb1PyEe9FHgWmD3dGVDF3X5YDicV+Mr5YQxJRLDQ37qD+lxTAhPcTNa4MBqX6s\n1cs9UcRxDhtupB3HcdaYUXdR56s+CW0VrKyetM1im2TheHEGXAirk8misQp2jGsSlycC9fFlcuzc\nLF3LWo/BTjzp4y1S5zl1XLs8VbvZTliG2G3HcfaCaau5ovTKOrOkjFMqLVu+95gCF0vHmtKqFShS\ntS/EUIqht4ZabgaxLm2123laZl2Jzyqd13Gc/aDJmc0udziOc+ixnYyaxIo86SJdO3wP04EAtv2U\nUNWSqipu1u6XivfVmYmyrMl3XMdx0kwb7I+uZOQiJ+hQuKHq0iJSRFhX1N6wCQlB6hjTYxp1UtHG\nX+p53LTxlYmRuEF2nKNAk52v5t5eHMdxFsSN9JK0mOZV6qTQ0oB+FKVRbBtqP4s3vc0xgDwV2U7+\nhRTgohnAKOsufmZGMg0ZyinfNtlFxizYCUhZpuO3uyppBqrlmlS0ia1FndpGn7dK8tHp5nY/WyY2\ntX25vnfcJEFXIYS6OHTHWS2pMgVNYWUheLrPIZRD1lqUQ9fqCunryAwtm0jT2JQmLS8xqNq4QxG6\nB3EYYCoCpcOIXtZ0wN5odF0OOauWZ3TWpQ77E/TNxSK/S4vixhFugMWNRmp5SJZmsU+43nMbm6Sj\nTqpKkNoMRzfOznrjmrTjOM4a43LHkpQL/cdepqbqUb/FtKJWdLHlMiM6uML1cq79qnMdGifUr2/u\nP1jH2QlupJdkqHoX2marUNaLdb/CsD7IBW+aFcWY9ONMqtxm6gYg6HohuuymPpb37XOc5uJx0o7j\nOGuMa9JLYr1aHTkQlpe7gUsX7TEdhvQib1yo9nYXfcSvKu3pOE6TcbljSWyt56rOJFav1vvYbVpM\nOaM6KmqJw8ofUMgstp9iahs5j47C0OOWfVJdXfQyHTJoI1H6DLhlI454cRxnb0hlMTeF5j4DOI7j\nLIhr0jsgLkVqS5Smy3ymllV5u7rbio4NttuRxQ7LufU5UmOyXr9e1zHXZZNAdKyyHEvGNKDP2Vnh\ngaciYKoSVHRc9JhO1OMQyDvXVDGl5aVRnUPNUdWkbwF+BHga+BTwWuDvAe8CrgUeBl4FXLA7julE\n5UrnZQNpQwvlJqaCbjgqTWpHyljNa4TaZWx07UW07J3coZdpm2WXpVpwVf1+U7WN4xxdmqxJX7LD\n/U4C/wJ4MfAighX4YeBm4B7gecCfZt8dx3FWin56rnutIzv1pP8OeAroE9y1PvB5gnd9XbbNO4B7\nSRhqLVGEyI1BFBedSqvWk3aS0txmmqdV2zhqkRzs5F6xfkpPSQcynltn5YnEuknI+E8cp5Xr49hz\nW6lHX5ukjENc1U/vXz5mMW5ZJhUA5YnCykq67ok8WQj2qUN3zUn9bar+vrJN1T5a8gm/R7qmiFyj\nnXS112Lj6m2MfOrv2lG/txBS5Z3DwlHUpL8A3AH8LTAE/oTgQV8OnM+2OZ99LyE6KhQGwDantdvr\nWhPyn9GWJRXDpI2OfJb/kCn91hpqaXCrjytjle3kxmJbbunzp/+jz5M4qrIRF9nfMu84+zXjvcxx\n52V7Lio5VZ2zbn9Z19yZf2cxdOu9prFTI/1s4PUE2eNLwH8m6NOaWfYq8f6tD3EpTwFw6vQJrj79\nnB0Ow3Gcw8VDhOmsvWVdpYxF2KmR/kbgz4Ans+//FfhW4HHgiuz9SuCJ1M43bH0Tg8xzlsfYTUaR\nV2rjocNgC49aRIFYjIhlj+L4xbEG9BnQj5aJ56urxGlZQUdkQNGkIBX1oav76WiNFDbmWi/XY9cy\nivba7T88vX0q9tv+FnEPR8dZB05lL+G+PTlqk+WOnU4cPgB8C+E5dQO4HrgfeA9wY7bNjcDdux2g\n4zjObpnmra7rX+vITkf1SeCdwJ8TQvD+EvhNYBO4C3gdRQheiYHSn8dZbWLtBaYKzbeYGk93kv2s\ng1ybFs+0x4B+lkaebptVR5WGueiyZdbrbXZy/GW39wp4ztHkKModALdnL80XCF51LdLVI46cEMNc\nTmzRURWFoW4xJvXI3iLcK3x23nGcwFE10o7jOI3AjfSS2NrPOoVZZwACeQ9EHcerJwFuyuJHdFyv\nPraNkxVZZJSF5EjInF4v59DjFHlFh/fZfok23tj2S/Sa1I6zGkZHMARvV8S99sqJBIJNztDJJ9qw\np9LGi2PEiSLSh7DH0MgqRfKLRRthOab+o9smBbGUU9TxODerjrSoutOnEkHkHHpc9nh636oIj1Si\njT6HboBg5wmqzjfPY5F9/IblHCS79KRvAH6JoKX+FnDbXoxpUVzucBzn0LMLI90Cfo0w1/YY8DHg\n3cDf7M3I5rPSetI6skOTiv/Vn6e08iJK1nvWnnOPIZts02fAv9o4ycH1MVyE3dzZp8QdupeJ2piX\ngafPATvL5Jt3fsc5WHYRJ/0S4EGKDJvfB17JYTfSQpWGrGs56FRwrfnqmhcWvf82m1zgOGdnxb66\nKL+X6HScw88uYqCvBh5R3x8FvnnXA1oClzscxzn0VMkdD9/7OT537+fqdk2WtjhIVhTdEVchS6VG\n64pnU9p5ASaZbNxkO/esbbSFPo9+7zDmmKqWBrA1iyM4wvbFxF8oyBQmN8dZXTk9aQeQqsqmiz3J\nGEbK+9fb6uJRdrJUX4uu7Kar5elkHnlSsL+pTgCyY07Fpeu/U7yunKKemnxMTYra9xZTn0B0DoQq\nI33i9Fdz4vRX598/8Asfsps8BpzQuxC86QNjpZq0jbzQhkpXxZMsQjEwI7pcyKrXpaIkbBicNj5j\nugzVNqnIEG14tGHUBKMZnxMo9THUVfZ6WeUQyZ4Uo721saxWXlctzxb6X8YILqpXzzvGMudznP1n\nXmORGv4ceC6hmNzngVcDr9mbUS2Gyx2O4xx6dqFJT4AfJ5RjbgG/zQFOGsIK46RtZ+3wuN6rLDw/\nzDxSGyMM5bjoCS3u2Fgk+mE6Zz0s75F2qI6I8FR1x1kFu4yT/qPstRJWYqRP8EiuJ2u0Tmq1Vdk2\npT1rmUTYyuT+qnA+q6PqKBI7Jn1TEFlmSnsHxZscx1kFuzTSK8XlDsdxDj1Nrie9EiP9ea4CUsXt\ni2iN8DlORx7Ryb1ZmYCTqIlhJpXY7t/6mB4T7ThHk3WtFb0IK09m0QWIbBPS8LlTmpmdqqgJvX0w\nysN8G71fmylnZkXGoh6DPbYdn24Ma8PtUmhpRs6dOpatg50qBJX6nfSNTI9Vf7f719UJ0ZEpfQZ5\nfRMJOZT+cPqGV1ULRP/mcpNMSUz63B6G5+w3Lnc4juOsMbatXpNYuZEWmUJ7m7rq2pRWXq7UotO/\nxbvTFfaOc4Futo14hDYhRO8rx9Qe45Ae2xxLTigK4ajjUgf01Jilel7KI9bjsDHgqUSQVNJLFXWe\nhD6H9IBMjcM+ZcTedJh47TBWcd/eCcZZD1yTXhJtBMMg4sd8QQymdHIRXXqUP36XS4hO0GVEN/Pt\nhCI0TxJzJyoGAAAbw0lEQVRCyqMrkzKCNsSvKrlkEerC/xb5xzVvu90mmMwLUVynwlWOU8Y1acdx\nnDXGNeklEUlCPGVIxS2Xk1zaTEuTg/IeR4jEE2X689aseDTXtSdSkSa2fojGnju1LlXvYqzkjnmT\nlnadlTesTKInKj2SxXEK3EgviQ25kwxEK11ItEBhnKuNkpAynlbPlTGkiLftlP64Wou1ZU9lPxmX\njFmuSyIfrIHW16NLsur1sdGtyoKcr007zlHENWnHcZw15qhq0scJ/b5eSKi5+lrgM8C7gGsJnQxe\nBVywO9pJwtB7MJYF5EftmggM8cB1ZIa86+XFcVrcupEKv9npRFuqe0mVR1s3oTivlojjOHtFk0Pw\nLtnFvr8M/CHwfOAfAg8ANwP3AM8D/jT77jiOs1ImmZw477WO7NST/vvAdwA3Zt8nwJeAVwDXZcve\nAdxLwlBrvVUKFskEnY1lDjWY4yxCyfiT8Lywrp3r2zbG+NxsnO9r08Z1FqDu9F010WAn7ew1VenN\nVcdJad56X6276xhyfc1Wa5d61vqa5Jqr2pDJ79Ch+K2G9PM5AXlKWb72teOsnqMod5wC/i/wduBr\ngb8AXg9cDpzPtjmffS8hE2hiSIqSoxILHSIguowyox0ngNRhI0M0Xca8sVTCdNn45kXjlne6Td1k\n4CKPbK0Ft7Ms0kTADbTTTI5idEcbeDGhGPbHgF+i7DHPqOgP9mdb/wOAS3ia55y+iq85nbTljuMc\nOR6iaMy9dxxFI/1o9vpY9v2/ALcAjwNXZO9XAk+kdn751ouBwnPeppAJegw5zoV8AlDC86a0Mx87\nTtseKXkECvkE4NyGLbLf3D+U4xwNTmUv4b49OepRNNKPE9qcPw/4NHA98NfZ60bgtuz97tTOugFq\nqs+grh9hf9yLlBumaqM9pkOHoL/eOtvObwQptH5sY5M1XqXNcZpNUSqieexGTf8J4HcJAuhnCSF4\nLeAu4HUUIXiO4zgr5Sh60gCfBL4psfz6eTva6m5xWveELqM88qBqP93jMLUtFBOSvayinpxP3q03\nnorKmNLi7KyoBS0ed6oWtN4Pis7hfYal6BKJpACizuHxtRZRHClJR6esp35HG+Fh/6HWPc3o38ru\nI+Oz+BOHs64cVSO9Y3RB+pDIIiU+24zpclEZJGtIw3swXlU1NYYmCqG+jsWikRh1USBVVeKkKW2q\nAe1uIiXq9t3JP8ZFGvLuxXkcZzWsawz0IjQ3eNBxHGdBjmKc9K6w8sCQfr5MKkaPlZwg+9hCSR1G\ntIyUIY/uIjUAnJuVI0CEukSTRSrR2c9aJrHXW8g55UYEE1r0GdJnkI9vQI8x3Yq0dsdxFsXljiXR\n+qxuGAvkWW5QdDwB0W2LJJgW0zxKRC8vivqL1JBintHbq4QVve287afUj9lxnJ3iRtpxHGeNGY2b\n6/ysTO6QiAdBywjS01AmAcXLbhNHYuiXSAY3zeI6F0XSS+FtSz/CqkiNFFXyhZU3UjU5qra3NbRl\nPy3Z6BR3GzUiv5NO6NG1OyaZlORRF85RZzpprj+6kpGLVqwL5utIjT6DvLCSGLYxnTzBRWZqxyps\nrcOYY5lxEqM1yKIgpGmAbBs03qo76257Ci7TI3BeqdM6uaZu/M31GhxnP5hOmuuoNPf24jiOsyBu\npJekz7CUwCECwJQ2A/pc4LhaXpQkBV0trxNFcnQZZxOSYS8tj+j3s6rsk0sBjnP4mTzV3P/nKzHS\n22xSZNMVRrqdvSZKG+7mgWpFFuIIeeAvMu+gKIEqYXw2E0+y/6R+9Igut856eZRJVQafjK8Oq0FX\nUVULuirbryp7cEzHbzCOsyBPT5srGjR35I7jOIvicsdyaK9T9yrUle+kE4is22Yz6iRiPde6BBWJ\n7hjQ4wLHSx2+ZRxFv5LCy5eoEcFGY+gx6PPpxJvU/tZDtzU/dFKLjivXSTnnZmNPdHGcRfhyc/3R\nlY3cZg8KtgzpSBllXUBIGzAxplq/1lmIentduCglIaCOs8l2PgYdoqdrVuvCRVVtq/Q1y7YyXt2+\nS67VhtPJWENN7bjQ09ZsmhybvpnIjcKOR+YA5oUQynJ7c9XZkxIKWPyNiiQlfZPx9lvOSpjM32Rd\nae7txXEcZ1HcSC+HjtQYlyI80skgOkFDR3cAUVq59UTlXXuw9lxlyUDOnfL66nr9LSs91FXXq6u6\nV1eNb7ee6qLx3+4ROw3CjfRy6IJKOmMwpf+CGNSWUp0D3RodumpZSk8+O6vPEkxFZFRFZaTOWyXp\n6PVau07VdrZjT+nyVbSYuszgHG2eWvUAdo7LHY7jHH7qI2jXmhXJHePSZJ9M0ukJLtuZRDzHnirp\nqSelZNKq7BFPSlJJMZZYRrETaK3Ec5KdPISybCMx2bLNOIv4Bl1bJF6nu8vIuGx9kfi6pkqqaW6I\nkePsOy53OI7jrDFfXvUAds7KjLRN9ZZuvjacLXi0Y3Rxfwge94jNyHvtZ224yv0Hgz86pWUmCfer\nGJEcN9U2a1H2oua14ziAe9LL0mfAgH4kEUCcCj3OpgVtbK7EFMskm463jmOlg2E/t7GJGzTHOeI0\n2Ehfssv9W8DHgfdk358J3AN8GngvZFWSHMdxVslkwdcasltP+qeA+yme628mGOnbgTPZ95vtTgP6\nQCx56PC2DmM22c4L8w+yqcIR3VwWCfu0cu9aPGc9CTemw5lZXEnPZsjJZFxHSSqx9x6Pz2Y2AlGf\nwvCjTud0KHcc50A5oiF41wAvB/4D8DPZslcA12Wf3wHcS8JIDysSISTCYkCfbY5F0Q2pbiUA2xzj\nSS4DyhERNlJkQK8ykmNER/VH1KS0YSnib9c1OM7HcQ4zDf6vuRsj/VbgZ4GvUMsuB85nn89n3x3H\ncVbLmkoZi7BTI/39wBMEPfp0xTaz7FXiOBeSmYW2lrPQURHCOu5ZIjpSBZB00SKbJt6BXJ7Q2X3n\nzGhtpqCVROJ46zj7MS4GlY7F1mPS50tlPmpS8dJ1mZCp40mRpqqCSuG360RPIpJ+rxs2yLGAvKei\nNGMo4tlHeXy6VDqUv538pmHLPmM6LhU5e88RDMH7NoK08XLgGQRv+ncI3vMVwOPAlQRDXuKPtv6c\nDWZcwtOcOn2CU6dPlLYpqtaVdWKQELxYU9bGQoxBOEZh5LSBszU/dGp2nyE9BlGz1yG9fH2XMT2G\n0RgBk35dJYssw7zIlLpaG1X9FmFva2/YY03V8ro6I15m1bE8BDy894c9gp702ewFQYP+N8A/I0wY\n3gjclr3fndr5uq3vzE2i4zhOwansJdy3N4dtsKnZqzhpEQreDNwFvI5wO3xV1Q6pIkRWPqhLzW7n\n/nWI6BjSyycbbdF8eczWNZxTLalkHIXs0GaQLdf7SpSIeNgXs4YEAGdn8cSmF+V3nDXgiBvp+yhu\nd18Arp+3g9ZedeQGFNKELSlaaKFF/YteVhujMJ6FZj2iG+mfQCRPdLOjQajKJ5poi0n+TdfN0OMe\nZok4sqwVGfv45nNmRiSR6GYFKW05dWOytUxkmdXuF/nNbZRMVeH/Kq28XEI2bkKgZSFbzc9vWM7K\nOKIheI7jOM3giIbg7RgdCWGx3pyuhBciCcaRd2Y9Wuvt9RiobuISBdJjqLw8HW0gHm4RKSIpNKM8\noiFcw6g08ShevvVMxYOW2iLiccq5tVdb7F9Mlha/SdGaKnjA1U8i+jfRy+Wc4sWnIykWTaOva1ow\nVdt4Wr6zYo5gdMeu0RERi04gakNT1cdPlwGV7MPtLCFSHs9le8FmIhZ1QdpZ7Y8Uyzy61xmzZY5V\nZeymNdu4gXSco65JO47jrDeuSS+ProA3zTxMHTmRSoCQbXQiho1vDkJAD90oIJ1UEk+G2Vhs2W9r\nNowmxnQkiLekcpyG4Jr0cgzpZQa5nKV3QRXO01EIYrT1Ni2muebcZhplHsr6Y2zn2rHoxnKDsIYb\nUtl8k1yL1kWUuow5NyuPX2MjNgQZo42AsIk18tkez2Yw+s3CcebgcofjOM4a40Z6OfTknfU+ZQKv\n7FV28giJ41ygyyjqWSixzJLKLZEUECQQ8Z7bTOlm/RTfsHEZ9ZN2u41amLfNshEQe5Fm7jhHENek\nl0NKhsbRGtM8gUS2scV6WkzytllSnlTQWjXAj2+U64E4jnNEGc3fZF1xucNxnMOPyx3LsclFoJxu\nLPUxoJgU3GQ7lzWGWUeXDmOOcyGfQBvRZZtNLnCcCS26jHnT7EI+qSjrdcU87bXr2h5y7k222WSb\nHsN8/SCLGtF1PyDIKdXx1I7jrByXO5ZDd2bRkQ2S4DJWFaQlLC/EQoSthvQig6617T5DWlnkwwX+\ngQrBm7Cpakxf4HgUBdJSkoqwnZlqrX3rkL5gtoP2fcdsyIBeqbmujeyQsXrNZMc5QDwEz3EcZ41x\nuWN3aNkjFS88pZ3VqBvmy0WesN6wSCApJsSxx3VlS7WsYcuiyr4TWmyzGXnZ+rj6+mzc85lZnLwD\nRNdUFzutrzd81jU/WmYc6ZT3+Pcv/hnoJwmXcJxDgxvp5ZDyRpqqRBDRqnWSiTY2NhlknJmklOQg\nBkhnKg7plSSX4jzt3CjLfraEqc2EBEryiKATeIrzh8KothCSLtgk1ylGvRhbXLpVX4c9vv19BZdd\nnCOBa9KO4zhrjIfgLYeezAvf28ojHdFnmJcGlYgKIKtpFzeD1fU05LvIDbK9nFNSwuuK7cvEX7mq\nXpwirpF9R2pffWw5vx6f9XjLzXL1KHUETJe+StgRjz7sWzwFaFlGmhwUv0Ox/e2qk4ykvevqhFoG\nKsYSd7GxMo+nqTtrh8sdjuM4a4zLHcsjxfch1p9FQ5XlQsrblM92Uk57eINM7y0mAQd0GEUeM5BP\nOOrjaXRquT533cReFVUeqL3WWCsvvN0LHF9TLVnqZjvOmuEheMthE0vS0RWtyLhqo6er0aU6kkC6\nLogcX+83yR7/pb+hNtR620LyKHotjrPuKqnkFvtdSwR6IjPV91Bkk9Sx5VrPzeIJxlRlP4BUN5lO\n1kRX4rrlmDqtXss9+nfTv9163igcJ4HLHY7jOGuMG+nl6DKKqtQJ08xz0/WdNbqIkvYe7YScbCvH\n1PSMhNKiBabnouyj+ynqSc5O7o0Oc69+RDefqDyWTSXrSUrdqMCOS9eXBvGwy9PRen/ZT8YpmZZy\nbDm+/p31JGXwnMdRXWp7Dv3btpm65+w0lyOoSZ8A3gl8JTADfhP4FeCZwLuAa4GHgVcBF+zOOvFE\nurCIsQiV73TXllb+SF8Yz1FuYCRSIUXRsLWIWV7O0CxaQnRKIXp1KJcSXbQk6TzhrGr/ZZu92u3t\n+B3nkNHgELxLdrjfU8BPAy8EvgX418DzgZuBe4DnAX+afXccx1ktkwVfa8hOPenHsxfAReBvgKuB\nVwDXZcvfAdxLhaG2kRyyTLCygJ7Qs/HIsr2OO7YyhzzSv35WTNjZx3p7jBHdfOKux9Djfx2nqRxB\nuUNzEvh64CPA5cD5bPn57HsJbQirakmIIe1liS066sOmW2udtpPQgyXETZ/Xjic+xjh/F6llm2Oc\nmZWPY6MgoIju0KF1qSQXrXOXf4NWaT99LXqdTg23v0ccTRLflKSsqyegOIeeIxyCdwz4A+CnIOtJ\nVTDLXiXu2/oAT3MJMza4+vSzOXH61C6H4TjO4eAhwnTWHrOmUsYi7MZIX0ow0L8D3J0tOw9cQZBC\nrgSeSO143dZ3mmp1TwJxjLT2AK23bRM8xNMcZ5N2NvVattGJ1rZYkY0S0bQTnqlETdgO5RLdYeOU\n4zFNsvGWk2Ps9022c89ZYqilHneHMZtsczw/VxwvbeOs5fe9dUN+hymegOKsF6eyl3Df3hz2CBrp\nDeC3gfuBX1LL3w3cCNyWvd9d3lUMZlGtzsoA8hkKw1t1HGuo5TFeR31I+JvN6tN6uGZkzmcTX7bZ\nVOOdRCGDk6whQUdp3uXQuDibUI/dXp+cSx9HQhBHWTcaGUcqMUa2l6ShFhO2ZsN8vZZmUuGBKfR2\nIeqmnd94Uskvwh0bDX7mdJrNEdSkvx34EeCvgI9ny24B3gzcBbyOIgTPcRxntRxBT/pDVIfvXT9v\n5022c29QP/JL7DRQu1x7gIJO0+4yYpOLkaxgJxs12uuTycqqST3tgcr2qVrSqSgTm7J9y8bxeT+V\noSq2Wa9PSRd1ksaiMdzLjIOKZY7jLMtKMg4/z1WRNgvBGOpiSIIuut9nkGf7ieo6okObaRbMN8x1\nW12OU5dCraqHIQzo58fUcoTcBAR55B/QZpvN/OagZQ59MymiQtr5DejMLK7rYUMD9TFS2IgSWWbr\nb6Sa58o+LkE4znqz02QWx3Ec5wBYiSe9yXbu5YVBFAKBjkyA0P1bPNkw7VV+bLfL7SRW10wYSt9x\n3UBAt6LS28rEGBBJFVrWEEZ0k2295FhWpkl50TKmsE+bXtbX0TZIkGMVnrD2iBeRI+w+jnOYae7M\n4UqMdFG7o/gO5QQO/dgvhlaW6QarQBRlIPKCHOsNG5eZEVSFnqUMWp3mukitjdR+VcdYNhzOjazj\nLEZzZw69VKnjOEcA96SXYkCvJAdAuv6G3UYe8weZx5kqiN9hnCeRjOlw06xcetTGZ+tjT2kn+yPa\nY3gqteM0heGqB7BjVqRJF+FxogdDEaJm9Vet+3YY01MRIKltypmFMTY8Tm4GEnonY9Iasq3XPKbD\nmSzpvSoDckqLN24sU0LUcZz9wT1px3GcNcY16aXQtSc6jDjOIIpHtskgti8hlFOXbco3lPv26WPb\n7UOaeC8/ay+rzSGJMCLRdBKSie4Yo2OTO4y4fTbOzz3MYrDlWopjletrSPKNTR+fV39EsPVHNFVJ\nPYukg9sKe6nfOfwdw+/mTxLOeuCe9FJcxpO5wQxGsMgU7BvJoaqUaXlZ0bi1r4z+lNAxXFpGhYoX\n2/k5Bpl4IoZZIkYkeUWMYp9hbhAFLbkM6C8ob2jjuUgXlLpMwOq6JvtD3bW5Pu+sM+5JO47jrDHu\nSS+FpF3r6nFT2ozoMqjo1KKXafmjwzgvGyo9Ei+ymRe015XjprS4yGa+vpwSXeeVHrTX6jjO3uHR\nHY7jOGuMyx1LEQoMlcPoBF1kSRcMKoocFfWYRScuCgnF2YxVtZq7jDk3K0+E2XC/rtKhi97m8eSf\nxcZ123hsjazT2ntV7Lcdo51QlHBBaTmmx1wcJ05tl/PZydC4yl84itS3Fo0foMcwnwMIE6z9KIY9\nNUGri04VDQgcZz9xuWMpOowywxxOb5NXZFn4jy/V7ArZomceXaTiXVV0ghg5HYEg20vSi2y3mXUB\nk3Vxgf8iIsMeP/4+oajJUX0jkIp+stxGbohJ0xOltpSrNBg4zoXot5OJTB0hog207iijjaZsU2zX\nSaTVa3qk0+uLEZeZ1qxznP3APWnHcZw1xj3ppdBtq7T3Vlcw3z7i62PpEDobwqfTwyUMTygevgvZ\n5QLHE9JLuq2UjXGGICHo0L3bNup+iXkTlYsyb1JTwviqttGerXu3zmHEPemlsDquxEtrtHFsJX5g\nMeV9Bkq2CAX4rayhjydabYvpnMf4Ys/FO5Y4jrOeuCftOI6zxngI3lIM6UWRCCI5iEygE8KntKN0\nap35p1OtdeW6CUVqtpwLyCMU5KULJNnoCR3hkGpvlYqqKCYWiw7iMsFXl3KdSrWG8uShjizxtleO\nswzuSS9FrCn3cl1aIivE+F5kM6t6N8ijF7QBlZA76W0oHVrGdPLQMOkCI9EOUK7zYY2ojiQJjCPT\nrSMkpN9ian04l0SyxKFz8m5DBLVhDsuLnohQlErdmhXb6BuBvlEU19hWn9MJQvraq5r+6siWKS0P\nn3MaRHM16f3ocXgD8ADwGeDMPhzfcRxnSZ5a8LUU/wT4a8LM+4vV8pMEfeXj2evX1bpvAD5FsI+/\nvMhJ9tqTbgG/BlwPPAZ8DHg38DfVOxQThDq2FwqvMkwIxhOL0hNce8W6aD/ANps8fO/nOHn62mSd\nah07bHsWShU8QXujNgrFTkpCHIesizeFycq6ichFJilbwEPAqYp1uyl2lIoCOci45qrrajp+Xatl\nXzzpTwE/CPynxLoHga9PLP8N4HXAR4E/JDi1f1x3kr020i/JBvdw9v33gVdijHSPQdR4dkQ3D8uT\nxIwuIyaEDDcdZmc1X92AVpIwdE2Qz2VGeqwSXuJwvjAKQcY0VPKIGNlUIoiWbnRiTHF8kQfaTJny\nptmFKHNRtPT6ML9Yf57S4gNbD/HtW6fybSypkEUtF01pzQkPXBUP04z/9MvyMH5dq2RfNOkHltz+\nSmCTYKAB3gn8AHOM9F7LHVcDj6jvj2bLHMdxVshkwdeecYogddwLvDRbdjXBJgqPsYB93GtPerbI\nRuLvivwQ/OE4oQSKjuB9hpnHXdR4Tqc5x3WpwzFClTxdw0JqW+iIjpAG08vGE3vOEqVxLjlRtkwc\ndV138qpjVB1/gw/9gt6mCo/zdpxdhODdA1yRWH4WeE/FPp8HTgBfJGjVdwMv3OkA9ppvIXbdb6E8\nefgJgjH3l7/85a95r0+we5Y539/t4PjvJ544rFp/JbH0+xrgP+7gfLuiDXyWMLvZIfzAzz/oQTiO\n4xwg7ydEbQjPonh8/WqCxHE8+/4R4JuBDYqJwwPne4H/TZhAvGUVA3AcxzkAfpAwBzcEHgf+KFv+\nQ8D/ImjSfwF8n9pHQvAeBH7lwEbqOI7jOMvStKSatwHnCXdZ4ZmEiYtPA++leGSC8JTyGcI1fo9a\nvnSw/D5zgvA4+NcE7+Ins+VNv7ZnEB5dPwHcD7wpW9706xJaBE9QJscOy3U5a0KL8DhxEriUZmjj\n30EIftdG+nbgDdnnM8Cbs88vIFzTpYRrfJCgcUGIwXxJ9nllmpfiCuDrss/HCFLY8zkc1yY1d9vA\nhwmhVofhugB+BvhdQjIaHJ7rctaEbyWOMrk5e607J4mN9APA5dnnKyiC523UzB8TImvs7PEPs4LZ\n4zncTchIPUzX1idk176Qw3Fd1wDvA15G4UkfhutqJPtRu2MdOCxJNZcTJBCyd/lPchVxULxcn12+\nULD8AXKS8LTwEQ7HtV1C8CLPU0g6h+G63gr8LPC0WnYYrquRHFYjPVv1APYBieNsKseAPwB+CrJy\nhwVNvbanCVLONcB3EjxPTROv6/uBJwh6dFXhgCZeV2M5rEb6McKElXCC+K7eFM5TZDtdSfjPA+Xr\nu4ZwfY9ln/Xyx/Z5jItwKcFA/w5B7oDDc20AXwL+O2GirOnX9W3AKwiVk+4Evovwd2v6dTlrRlOT\nak5SnjgUve9mypM1HUKNgM9SeD1rESyv2CAUknmrWd70a3sWRYRDD/gA8N00/7o011Fo0ofpupw1\noWlJNXcScv7HBD39tYSwp/eRDns6S7i2B4B/rJavW7D8SwmywCco6uveQPOv7UXAXxKu668IGi40\n/7o011FEdxym63Icx3Ecx3Ecx3Ecx3Ecx3Ecx3Ecx3Ecx3Ecx3Ecx3Ecx3Ecx3EcJ8X/B2ZFoqN7\nJTW2AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fd7ed32cc10>"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eidx = active_elecs == elec\n",
      "data = data_all[eidx, :,:].squeeze()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eidx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 122,
       "text": [
        "array([False,  True, False, False, False, False, False, False, False,\n",
        "       False, False, False, False, False, False, False, False, False,\n",
        "       False, False, False, False, False, False, False, False, False,\n",
        "       False, False, False, False], dtype=bool)"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "st_resp = 0\n",
      "start_idx = start_idx + abs(bl_st)\n",
      "end_idx_resp = end_idx_resp + abs(st_resp)\n",
      "\n",
      "#create data matrices\n",
      "data_dur = np.empty((RT.shape[0], data.shape[1]))\n",
      "for j, r in enumerate(RT):\n",
      "    tmp = data[j, start_idx : r + end_idx_resp]\n",
      "    tmp = np.pad(tmp, (0, data.shape[1]-len(tmp)), 'constant', constant_values = -999)\n",
      "    data_dur[j,:] = tmp\n",
      "data_dur[data_dur == -999] = np.nan"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 305
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#calculate stats (single trials)\n",
      "nanidx = np.isnan(np.nansum(data_dur, axis = 1)) #if start > end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 306
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if np.any(nanidx):\n",
      "    num_to_drop = np.sum(nanidx)\n",
      "    i =  np.argpartition(RT, -num_to_drop)[-num_to_drop:] #find the indices of the longest RTs\n",
      "    nanidx[i] = True\n",
      "    \n",
      "    data_dur[nanidx,:] = np.nan\n",
      "    means[elec] = np.nanmean(data_dur, axis = 1)\n",
      "    stds[elec] = np.nanstd(data_dur, axis = 1)\n",
      "    maxes[elec] = np.nanmax(data_dur, axis = 1)\n",
      "    sums[elec] = np.nansum(data_dur, axis = 1)\n",
      "    \n",
      "    data_dur[nanidx,0] = -999\n",
      "    tmp_lat = np.nanargmax(data_dur, axis = 1)\n",
      "    tmp_lat = np.ndarray.astype(tmp_lat, dtype = float)\n",
      "    tmp_lat[nanidx] = np.nan\n",
      "    lats[elec] = tmp_lat\n",
      "    lats_pro[elec] = tmp_lat / np.sum(~np.isnan(data_dur), axis = 1)\n",
      "    tmp_RT = np.ndarray.astype(RT, dtype = float)\n",
      "    tmp_RT[nanidx] = np.nan\n",
      "    RTs[elec] = tmp_RT\n",
      "else:\n",
      "    means[elec] = np.nanmean(data_dur, axis = 1)\n",
      "    stds[elec] = np.nanstd(data_dur, axis = 1)\n",
      "    maxes[elec] = np.nanmax(data_dur, axis = 1)\n",
      "    sums[elec] = np.nansum(data_dur, axis = 1)\n",
      "\n",
      "    lats[elec] = np.nanargmax(data_dur, axis = 1)\n",
      "    lats_pro[elec] = np.nanargmax(data_dur, axis = 1) / np.sum(~np.isnan(data_dur), axis = 1)\n",
      "    RTs[elec] = RT\n",
      "    print 'no nans'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 307
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(np.isnan(lats[elec]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 309,
       "text": [
        "52"
       ]
      }
     ],
     "prompt_number": 309
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(nanidx)/len(nanidx)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 248,
       "text": [
        "0.45614035087719296"
       ]
      }
     ],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df.join(pd.DataFrame({'num_to_drop':np.random.permutation(len(df))}), how = 'outer')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 323
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#testing stuff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir,'PCA','Stats','shadeplots_stim_wthduration_drop_duplicates.csv')\n",
      "df = pd.read_csv(filename)\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA','Stats', 'shadeplots_resp_drop_duplicates.csv')\n",
      "df_resp = pd.read_csv(filename)\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA','Stats', 'significance_windows_stats.csv')\n",
      "df_stats = pd.read_csv(filename)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_resp = df_resp[['subj','task','group','elec','start_idx','end_idx','all_criteria_passed']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df[['subj','task','cluster ','elec','start_idx','end_idx','all_criteria_passed']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.columns = ['subj','task','cluster','elec','start_idx','end_idx','all_criteria_passed']\n",
      "df_resp.columns = ['subj','task','cluster','elec','start_idx_resp','end_idx_resp','all_criteria_passed']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_all = pd.merge(df, df_resp, how = 'outer')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_all = pd.merge(df_stats[['subj','task','cluster','pattern']], df_all, how = 'inner') #only keep active clusters. add their designation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_all = df_all[['subj','task','cluster','pattern','elec','start_idx','end_idx','start_idx_resp','end_idx_resp']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir,'PCA','Stats','single_electrode_windows_withdesignation.csv')\n",
      "df_all.to_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##testing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SJdir = '/home/knight/matar/MATLAB/DATA/Avgusta/'\n",
      "\n",
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_withdesignation.csv')\n",
      "df = pd.read_csv(filename)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load data\n",
      "filename = os.path.join(SJdir, 'Subjs', subj, task, 'HG_elecMTX.mat')\n",
      "data_dict = loadmat.loadmat(filename)\n",
      "\n",
      "active_elecs, Params, srate, RTs, data_all = [data_dict.get(k) for k in ['active_elecs','Params','srate','RTs','data']]\n",
      "bl_st = Params['bl_st']\n",
      "means = dict(); stds = dict(); maxes = dict(); lats = dict(); sums = dict(); lats_pro = dict()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "row = df.iloc[9]\n",
      "_, subj, task, cluster, pattern, elec, start_idx, end_idx, start_idx_resp, end_idx_resp = row\n",
      "\n",
      "eidx = np.in1d(active_elecs, elec)\n",
      "data = data_all[eidx,:,:].squeeze()\n",
      "\n",
      "st_resp = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 266
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#define start and end indices based on electrode type\n",
      "if any([(pattern == 'S'), (pattern == 'sustained'), (pattern == 'S+sustained'), (pattern == 'SR')]):\n",
      "    start_idx = start_idx + abs(bl_st)\n",
      "    end_idx = end_idx + abs(bl_st)\n",
      "\n",
      "    #calculate stats (single trials)\n",
      "    means[elec] = data[:,start_idx:end_idx].mean(axis = 1)\n",
      "    stds[elec] = data[:,start_idx:end_idx].std(axis = 1)\n",
      "    maxes[elec] = data[:,start_idx:end_idx].max(axis = 1)\n",
      "    lats[elec] = data[:,start_idx:end_idx].argmax(axis = 1)\n",
      "    sums[elec] = data[:, start_idx:end_idx].sum(axis = 1)\n",
      "    lats_pro[elec] = lats[elec] / len(np.arange(start_idx, end_idx))\n",
      "\n",
      "if pattern == 'R':\n",
      "    start_idx_resp = start_idx_resp+abs(st_resp)\n",
      "    end_idx_resp = end_idx_resp+abs(st_resp)\n",
      "\n",
      "    #create data matrix\n",
      "    #data_resp = np.empty(data.shape)\n",
      "    data_resp = np.empty((RTs.shape[0], data.shape[1]))\n",
      "    for j, r in enumerate(RTs):\n",
      "        tmp = data[j, r + start_idx_resp : r + end_idx_resp]\n",
      "        tmp = np.pad(tmp, (0, data.shape[1]-len(tmp)), 'constant', constant_values = -999)\n",
      "        data_resp[j,:] = tmp\n",
      "    data_resp[data_resp == -999] = np.nan\n",
      "\n",
      "    #calculate stats (single trials)\n",
      "    means[elec] = np.nanmean(data_resp, axis = 1)\n",
      "    stds[elec] = np.nanstd(data_resp, axis = 1)\n",
      "    maxes[elec] = np.nanmax(data_resp, axis = 1)\n",
      "    sums[elec] = np.nansum(data_resp, axis = 1)\n",
      "    \n",
      "    nanidx = np.isnan(np.nansum(data_dur, axis = 1)) #if start > end\n",
      "    if np.any(nanidx):\n",
      "        data_dur[nanidx,0] = -999\n",
      "        tmp_lat = np.nanargmax(data_resp, axis = 1)\n",
      "        tmp_lat = ndarray.astype(tmp_lat, dtype = float)\n",
      "        tmp_lat[nanidx] = np.nan\n",
      "        lats[elec] = tmp_lat\n",
      "        lats_pro[elec] = tmp_lat / np.sum(~np.isnan(data_resp), axis = 1)\n",
      "\n",
      "    else:\n",
      "        lats[elec] = np.nanargmax(dataresp, axis = 1)\n",
      "        lats_pro[elec] = np.nanargmax(dataresp, axis = 1) / np.sum(~np.isnan(dataresp), axis = 1)\n",
      "   \n",
      "if pattern == 'D':\n",
      "        start_idx = start_idx + abs(bl_st)\n",
      "        end_idx_resp = end_idx_resp+abs(st_resp)\n",
      "        \n",
      "        #create data matrices\n",
      "        #data_dur = np.empty(data.shape)\n",
      "        data_dur = np.empty((RTs.shape[0], data.shape[1]))\n",
      "        for j, r in enumerate(RTs):\n",
      "            tmp = data[j, start_idx : r + end_idx_resp]\n",
      "            tmp = np.pad(tmp, (0, data.shape[1]-len(tmp)), 'constant', constant_values = -999)\n",
      "            data_dur[j,:] = tmp\n",
      "        data_dur[data_dur == -999] = np.nan\n",
      "\n",
      "        #calculate stats (single trials)\n",
      "        means[elec] = np.nanmean(data_dur, axis = 1)\n",
      "        stds[elec] = np.nanstd(data_dur, axis = 1)\n",
      "        maxes[elec] = np.nanmax(data_dur, axis = 1)\n",
      "        sums[elec] = np.nansum(data_dur, axis = 1)\n",
      "        \n",
      "        nanidx = np.isnan(np.nansum(data_dur, axis = 1)) #if start > end\n",
      "        if np.any(nanidx):\n",
      "            data_dur[nanidx,0] = -999\n",
      "            tmp_lat = np.nanargmax(data_dur, axis = 1)\n",
      "            tmp_lat = ndarray.astype(tmp_lat, dtype = float)\n",
      "            tmp_lat[nanidx] = np.nan\n",
      "            lats[elec] = tmp_lat\n",
      "            lats_pro[elec] = tmp_lat / np.sum(~np.isnan(data_dur), axis = 1)\n",
      "            \n",
      "        else:\n",
      "            lats[elec] = np.nanargmax(data_dur, axis = 1)\n",
      "            lats_pro[elec] = np.nanargmax(data_dur, axis = 1) / np.sum(~np.isnan(data_dur), axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 267
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dur[np.isnan(np.nansum(data_dur, axis = 1)),0] = -999"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp = np.nanargmax(data_dur, axis = 1)\n",
      "tmp[np.isnan(np.nansum(data_dur, axis = 1))] = -999"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nanidx = np.isnan(np.nansum(data_dur, axis = 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if any(nanidx):\n",
      "    data_dur[nanidx,0] = -999\n",
      "    tmp_lat = np.nanargmax(data_dur, axis = 1)\n",
      "    tmp[nanidx] = -999"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#combining"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir,'PCA', 'Stats', 'single_electrode_windows_withdesignation.csv')\n",
      "df = pd.read_csv(filename)\n",
      "df = df[['subj','task']].drop_duplicates(['subj', 'task'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subj = 'GP15'\n",
      "task = 'EmoGen'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = os.path.join(SJdir, 'PCA', 'ShadePlots_hclust', 'elecs', 'significance_windows', 'data', ''.join([subj, '_', task, '.p']))\n",
      "tmp = pickle.load( open( filename, \"rb\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in tmp['means'].keys():\n",
      "    print (k, len(tmp['means'][k]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(5, 136)\n",
        "(9, 147)\n",
        "(10, 143)\n",
        "(12, 146)\n",
        "(13, 147)\n",
        "(14, 147)\n",
        "(17, 147)\n",
        "(18, 146)\n",
        "(19, 147)\n",
        "(21, 147)\n",
        "(22, 147)\n",
        "(23, 147)\n",
        "(25, 147)\n",
        "(27, 106)\n",
        "(29, 147)\n",
        "(30, 147)\n",
        "(31, 147)\n",
        "(37, 147)\n",
        "(38, 147)\n",
        "(42, 147)\n",
        "(44, 147)\n",
        "(45, 147)\n",
        "(52, 147)\n",
        "(53, 147)\n",
        "(54, 147)\n",
        "(63, 147)\n",
        "(66, 141)\n",
        "(67, 147)\n",
        "(68, 147)\n"
       ]
      }
     ],
     "prompt_number": 127
    }
   ],
   "metadata": {}
  }
 ]
}